# Day 41: IPAS 中級能力鑑定考題解析 (114年第二次)

## 1. 考試資訊與來源
*   **考試名稱**：114 年第二次 AI 應用規劃師-中級能力鑑定
*   **考試日期**：114 年 11 月 08 日
*   **試題來源**：[IPAS 學習資源專區](https://www.ipas.org.tw/certification/AIAP/learning-resources)
    *   *註：試題中的圖片與完整排版請參考官方公告之 PDF 檔案。*
![41-1](pic/41-1.png)



## 3. 試題彙整 (第一科：人工智慧技術應用與規劃)

1.  **情感分析目的**：(B) 判斷文本中所表達的情感傾向。
2.  **Transformer 優勢**：(A) 透過 **自注意力機制 (Self-Attention)** 捕捉長距離語境依賴關係，解決了 RNN 難以記憶長句子的問題。
3.  **BERT MLM 策略**：(B) 隨機遮罩部分詞語，並讓模型根據雙向上下文預測。
4.  **GloVe vs Word2Vec**：(C) Word2Vec 基於預測，GloVe 基於共現統計。
5.  **TF-IDF 長文本問題**：(A) 長文本詞頻偏高，導致常見詞權重被過度放大。
6.  **N-gram 限制**：(B) 難以捕捉長距離依賴關係。
7.  **mAP 高 IoU 意義**：(A) 預測邊界框與真實邊界框的重疊程度越高，結果越精準。
8.  **Softmax vs Max-Pooling**：(C) Softmax 用於比例表示(分類)，Max-Pooling 保留區域最大值(降維)。
9.  **資料增強效能下降**：(B) 增強後特徵分佈不一致，影響泛化能力。
10. **Precision & Recall 綜合指標**：(D) F1 分數。
11. **DBSCAN 超參數**：(C) 鄰域半徑 (Epsilon) 與最小點數 (MinPts)。
12. **多重共線性處理**：(B) 使用 PCA 轉換為獨立主成分。
13. **Kubernetes 功能**：(B) 管理與協調模型服務的部署、擴展與運行環境。
14. **防止過擬合 (超參數)**：(A) 採用交叉驗證 (Cross-Validation)。
15. **Model Registry 用途**：(C) 集中管理模型版本、訓練紀錄與部署狀態。
16. **Seq2Seq 應用**：(D) 自動翻譯或摘要生成。
17. **RAG 檢索挑戰**：(D) 避免結果僅具語意相似但無實質關聯 (例如問股價卻找到食譜)。
18. **Attention Collapse 改善**：(D) 對注意力權重施加 **稀疏化約束 (Sparsity Constraint)**，強迫模型只關注最重要的部分。
19. **低資源語言訓練**：(B) 採用反向翻譯 (Back-Translation) 生成偽平行語料。
20. **GAN Mode Collapse**：(B) 採用 Wasserstein 距離 (WGAN) 或梯度懲罰來解決。
21. **多模態缺失處理**：(B) 訓練具備模態缺失感知能力的模型。
22. **資料漂移偵測**：(D) 計算 KL 散度 (KL Divergence)。
23. **漸進式部署**：(A) 從單一專科開始，逐步擴展。
24. **對抗性攻擊防禦 (非)**：(D) 強化防火牆 (這是資安層面，非模型層面)。
25. **生成內容侵權預防**：(B) 建立訓練資料篩選與授權驗證機制。
26. **共線性模型選擇**：(D) 含 L1 正則化的 LASSO 迴歸。
27. **JSON 日誌特徵提取**：(C) 展開巢狀欄位並基於時間窗口聚合。
28. **混合特徵處理**：(C) 連續特徵標準化，類別特徵目標編碼。
29. **CI 核心實踐**：(B) 提交後自動觸發建置與測試。
30. **不可否認性**：(A) 記錄雜湊值並簽署數位簽章。
31. **高流量架構**：(B) 容器化部署並水平擴展 (Auto Scaling)。
32. **效能衰退預警**：(D) 輸入特徵分佈的 PSI 指數。
33. **罕見詞訓練 (Word2Vec)**：(C) Skip-gram 模型 (對低頻詞效果較好)。
34. **個體識別**：(C) 實例分割 (Instance Segmentation)。
35. **CLIP 特性**：(A) 圖文對比學習，零樣本分類。
36. **系統化參數測試**：(B) 網格搜尋 (Grid Search)。
37. **GPU 記憶體不足**：(B) 較小 Batch Size 搭配資料分片 (Gradient Accumulation)。
38. **Stable Diffusion 清晰度**：(B) 增加 **取樣步數 (Sampling Steps)** 並選擇高品質取樣器。
39. **ARIMA 殘差自相關**：(C) 模型配適不足，需調整 p, q 參數。
40. **生成模型差異**：(A) **VAE** 模糊但結構好，**GAN** 清晰但不穩，**Diffusion** 穩定且多樣。
41. **交叉驗證資料洩漏**：(A) 測試摺參與參數選擇 (Over-optimistic Bias)。
42. **新樣本分佈偏移**：(D) 使用 VAE 監控潛在空間分佈。
43. **模型比較實驗**：(B) 低資源情境下比較 (Data Efficiency)。
44. **預測與生成兼顧**：(C) VAE 或 GAN。
45. **PCA + SVM**：(D) 降維可降低訓練時間並減少過擬合。
46. **MLOps 漂移偵測**：(A) 建立即時 Data/Concept Drift 監測。
47. **多任務學習衝突**：(C) 損失函數未平衡。
48. **DBSCAN 加速**：(B) 使用 KD-Tree 或 Ball Tree 索引。
49. **跨文化情感分析 (非)**：(A) 詞嵌入正規化導致誤差 (通常正規化是好事)。*(註：此題選項 A 描述較為牽強，但相比其他選項 B/C/D 都是常見偏誤原因，A 較不直接)*
50. **多模態細節錯誤**：(C) CLIP 編碼器在語意空間未充分對齊。

---

## 4. 試題彙整 (第三科：機器學習技術與應用)

1.  **模型穩定性檢驗**：(B) **交叉驗證 (Cross-Validation)** 是評估泛化能力的首選。
2.  **L1 正則化效果**：(C) 產生 **稀疏模型 (Sparse Model)**，達到特徵選擇的效果。
3.  **非凸函數最佳化**：(C) 局部最優解 (Local Optima)。
4.  **DBSCAN 孤立點**：(B) 雜訊點 (Noise Point)。
5.  **CNN 第一層功能**：(A) **提取局部特徵** (如邊緣、紋理)。
6.  **CNN vs FCNN**：(C) 區域感知與參數共享降低複雜度。
7.  **LSTM 應用**：(A) 預測電力需求趨勢 (時間序列)。
8.  **資訊增益應用**：(D) 決策樹模型。
9.  **距離模型前處理**：(A) 特徵縮放 (Feature Scaling)。
10. **AutoML 適用情境**：(C) 行銷部門需快速比較多種模型且缺人手。
11. **Random Search 優勢**：(D) 更有效率搜尋高維參數空間。
12. **改善收斂不穩**：(C) 調整學習率 (Learning Rate)。
13. **標籤偏差原因**：(B) 標記資料帶有主觀偏見。
14. **可解釋性關鍵情境**：(C) 醫療診斷 (腫瘤判斷)。
15. **R-Squared 意義**：(B) 85% 的變異可被模型解釋。
16. **F1 Score 計算**：(A) 2*(0.8*0.6)/(0.8+0.6) ≈ 0.686。
17. **動量優化器**：(B) Adam (結合動量與 RMSProp)。*(註：SGD+Momentum 也是，但 Adam 是更典型的現代代表)*
18. **XGBoost 改進**：(A) 引入 **正則化項**、支援缺失值與並行。
19. **不平衡資料處理 (不適合)**：(C) 使用準確率 (Accuracy) 評估。
20. **互動特徵工程**：(C) 特徵乘積或交互組合。
21. **多頭注意力優點**：(C) 從不同子空間捕捉多樣化關聯。
22. **貝氏定理機制**：(B) 以條件機率計算分類機率。
23. **隨機抽樣模擬**：(A) 蒙地卡羅方法 (Monte Carlo)。
24. **殘差圖彎曲**：(C) 存在非線性關係，違反迴歸假設。
25. **信用評分卡 (非)**：(A) 使用生成式模型特徵學習 (傳統流程較少用)。
26. **防止過擬合 (非)**：(D) 擴增輸入特徵 (這會增加複雜度)。
27. **線性激活問題**：(D) 改用 ReLU 引入非線性。
28. **取樣偏差**：(C) 訓練樣本僅涵蓋高活躍顧客。
29. **長期穩健性**：(D) 時間序列交叉驗證 (Rolling Window)。
30. **跨語言效能下降**：(C) 語言轉移造成 Recall 下降 (關鍵詞辨識失敗)。
31. **早停策略**：(B) 監控驗證集損失並設定 Patience。
32. **特徵篩選 (正則化)**：(D) L1 正則化 (Lasso)。
33. **全對全比對複雜度**：(B) O(n²)。
34. **小樣本驗證**：(D) 分層留一法 (Stratified LOOCV)。
35. **PCA 解釋變異**：(A) (6+3)/(6+3+1) = 90% > 80%，前兩維足夠。
36. **同態加密特性**：(D) 加密狀態下仍可進行運算。
37. **安全多方計算**：(D) 同態加密 + MPC + 雜湊 + 對稱加密。
38. **程式碼指標**：(B) MSE (均方誤差)。
39. **程式碼正則化**：(C) Dropout (隨機丟棄神經元)。
40. **矩陣運算**：(C) np.dot(v1, v2) 為內積。
41. **條件機率程式碼**：(D) P(A|B) = P(A∩B)/P(B) -> A_and_B.sum() / B.sum()。
42. **VGG16 參數量最多**：(B) 全連接層 (Linear)。
43. **VGG16 運算量最多**：(A) 卷積層 (Conv2d) (因為 Feature Map 大)。
44. **VGG16 敘述正確**：(D) **VGG16** 架構為 13 層卷積 + 3 層全連接，總參數約 138M。
45. **遷移學習凍結**：(B) 設定 `param.requires_grad = False`。
46. **PCA 降噪修正**：(B) 需使用 `inverse_transform` 還原。
47. **KNN 交叉驗證**：(B) 程式碼 A 與 C 正確。
48. **資料標準化**：(C) A 與 D 正確 (防止梯度問題)。*(註：題目選項可能有誤，標準化是 (x-mean)/std，A是減mean，B是除std，合起來才是標準化)*
49. **MLP 輸出**：(C) 空格 1 (Input) 為 100，空格 2 (Output) 為 110 (假設)。
50. **訓練曲線**：(C) A (藍實線) 與 D (紅虛線) 搭配。
