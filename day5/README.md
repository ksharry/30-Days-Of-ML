# Day 05：K-近鄰演算法 (KNN) —— 命運取決於你的鄰居

## 0. 歷史小故事：來自空軍基地的反叛
在 1951 年，德州蘭道夫空軍基地 (US Air Force School of Aviation Medicine) 發布了一份技術報告，作者是兩位統計學家 **伊芙琳·費克斯 (Evelyn Fix)** 與 **約瑟夫·霍奇斯 (Joseph Hodges)**。

當時的統計學界（如我們昨天學的邏輯回歸）癡迷於尋找完美的「數學公式」來描述數據分佈。但費克斯和霍奇斯提出了一個離經叛道的想法：
> 「如果我們完全不假設數據符合任何公式呢？如果我們直接讓數據自己說話呢？」

這就是 **非參數統計 (Non-parametric Statistics)** 的起源。他們提出的方法非常直觀：如果你想知道一個新數據點屬於哪一類，只要看看它附近最近的幾個點是什麼類別就好。這份報告奠定了 **K-Nearest Neighbors (KNN)** 的基礎，將機器學習從死板的公式中解放出來，進入了「幾何直覺」的時代。

---

## 1. 資料介紹與幾何視角
在 Day 04，我們用邏輯回歸像個嚴肅的法官，試圖畫出一條筆直的「紅線」將乘客分為生存區與死亡區。但如果這條界線不是直的怎麼辦？

今天，我們拋棄複雜的數學公式，使用 KNN 演算法：**「近朱者赤，近墨者黑」。**

### 1.1 Python 程式碼實作
完整程式連結-[KNN_Titanic.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/KNN_Titanic.py)
*(程式碼包含標準化處理與視覺化繪圖)*

---

## 2. 實驗結果：形狀的戰爭 (圖一解析)

我們將模型簡化，只使用 **Age (年齡)** 與 **Fare (票價)** 兩個特徵，畫出了這張對比圖。這張圖展示了兩種截然不同的世界觀。

![Decision Boundary Comparison](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-1.png?raw=true)

### 左圖：邏輯回歸 (Logistic Regression)
* **功能**：這是一條 **直線決策邊界 (Linear Decision Boundary)**。它將畫面一分為二，藍色區域預測死亡，白色區域預測生存。
* **意義**：
    * **「線性假設」的僵化**：邏輯回歸認為，票價越高、年齡越小，生存率就「線性地」越高。
    * **無法處理特例**：請注意畫面中間有些藍色點（倖存者）混在白色點（死亡者）中間，邏輯回歸的那條直線完全切不開它們，只能選擇無視。這就是「低度擬合 (Underfitting)」的徵兆。

### 右圖：K-近鄰演算法 (KNN)
* **功能**：這是不規則的、破碎的 **非線性決策邊界 (Non-Linear Decision Boundary)**。紫色區域預測死亡，白色區域預測生存。
* **意義**：
    * **「物以類聚」的彈性**：KNN 沒有公式限制。你看畫面下方，紫色區域像液體一樣滲透進去，包圍住了那些低票價的死亡群體。
    * **捕捉局部特徵**：它成功捕捉到了資料中的「小聚落」。這代表 KNN 能適應資料的真實形狀，而不是強迫資料去適應一條直線。

---

## 3. 深度分析：尋找最佳鄰居 (圖二解析)

既然 KNN 是看鄰居，那「看幾個鄰居」就成了關鍵。我們測試了 K 從 1 到 40 的結果。

![Accuracy and Confusion Matrix](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-2.png?raw=true)

### 左圖：K 值調參 (Accuracy vs. K Value)
* **功能**：這張折線圖顯示了 **K 值 (橫軸)** 與 **預測準確率 (縱軸)** 的關係。紅色虛線標示出了最佳的 K 值 (K=17)。
* **意義**：
    * **過度擬合 (Overfitting)**：如果 K 太小 (如 K=1)，準確率反而低。因為你太容易被單一一個「怪鄰居」誤導（雜訊）。
    * **低度擬合 (Underfitting)**：如果 K 太大 (如 K=40)，準確率也下降。因為你參考太多人，結果變成「大眾臉」，失去了局部特徵。
    * **黃金交叉點**：我們發現 **K=17** 是甜蜜點 (Sweet Spot)，此時模型既不盲從也不固執，準確率達到最高 (約 81.6%)。

### 右圖：混淆矩陣 (Confusion Matrix)
* **功能**：這是 K=17 時的最終成績單。
* **意義**：
    * **對角線 (深色區)**：預測正確的數量。**93** 人正確預測死亡，**53** 人正確預測生存。
    * **非對角線 (淺色區)**：犯錯的地方。
        * **21 (False Negative)**：這些人其實活著，但因為他們的鄰居大多都死了（可能是三等艙的倖存者），導致模型誤判他們會死。這顯示了 KNN 在處理「少數群體」時的弱點。

---

## 4. 總結 (Conclusion)

今天我們見證了 **非參數模型 (KNN)** 與 **參數模型 (LogReg)** 的對決。

* **LogReg** 是理性的：它相信世界有簡單的規則（直線）。
* **KNN** 是經驗的：它相信世界是複雜的，直接參考身邊的案例（鄰居）。

在鐵達尼號這個題目上，KNN 透過捕捉不規則形狀，準確率稍微勝過了邏輯回歸。但 KNN 有個致命傷：**計算量太大**。每次預測都要把所有人抓出來算一次距離，這在大數據時代是不可接受的。

### Next Step:
有沒有一種方法，可以擁有 KNN 的「非線性超強分類能力」，但又像邏輯回歸一樣「算出一條線就好」(不用每次都查鄰居)？

有的！那就是機器學習中期的霸主 —— **Day 06 支援向量機 (SVM)**。我們要學習如何將資料投影到高維空間，用「核函數 (Kernel Trick)」使出魔法，在四維空間切出一條完美的界線！