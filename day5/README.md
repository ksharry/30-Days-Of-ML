# Day 05：K-近鄰演算法 (KNN) —— 命運取決於你的鄰居

## 0. 歷史小故事：來自空軍基地的反叛
在 1951 年，德州蘭道夫空軍基地 (US Air Force School of Aviation Medicine) 發布了一份技術報告，作者是兩位統計學家 **伊芙琳·費克斯 (Evelyn Fix)** 與 **約瑟夫·霍奇斯 (Joseph Hodges)**。

當時的統計學界（如我們昨天學的邏輯回歸）癡迷於尋找完美的「數學公式」來描述數據分佈。但費克斯和霍奇斯提出了一個離經叛道的想法：
> 「如果我們完全不假設數據符合任何公式呢？如果我們直接讓數據自己說話呢？」

這就是 **非參數統計 (Non-parametric Statistics)** 的起源。他們提出的方法非常直觀：如果你想知道一個新數據點屬於哪一類，只要看看它附近最近的幾個點是什麼類別就好。這份報告奠定了 **K-Nearest Neighbors (KNN)** 的基礎，將機器學習從死板的公式中解放出來，進入了「幾何直覺」的時代。

---

## 1. 資料介紹與幾何視角
在 Day 04，我們用邏輯回歸像個嚴肅的法官，試圖畫出一條筆直的「紅線」將乘客分為生存區與死亡區。但如果這條界線不是直的怎麼辦？

今天，我們拋棄複雜的數學公式，使用 KNN 演算法：**「近朱者赤，近墨者黑」。**

### 1.1 Python 程式碼實作
完整程式連結-[KNN_Titanic.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/KNN_Titanic.py)
*(程式碼包含標準化處理與視覺化繪圖)*

---

## 2. 實驗結果：形狀的戰爭 (圖一解析)

我們將模型簡化，只使用 **Age (年齡)** 與 **Fare (票價)** 兩個特徵，畫出了這張對比圖。這張圖展示了兩種截然不同的世界觀。

![Decision Boundary Comparison](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-1.jpg?raw=true)

### 左圖：邏輯回歸 (Logistic Regression)
* **功能**：這是一條 **直線決策邊界 (Linear Decision Boundary)**。它將畫面一分為二，藍色區域預測死亡，白色區域預測生存。
* **意義**：
    * **「線性假設」的僵化**：邏輯回歸認為，票價越高、年齡越小，生存率就「線性地」越高。
    * **無法處理特例**：請注意畫面中間有些藍色點（倖存者）混在白色點（死亡者）中間，邏輯回歸的那條直線完全切不開它們，只能選擇無視。這就是「低度擬合 (Underfitting)」的徵兆。

### 右圖：K-近鄰演算法 (KNN)
* **功能**：這是不規則的、破碎的 **非線性決策邊界 (Non-Linear Decision Boundary)**。紫色區域預測死亡，白色區域預測生存。
* **意義**：
    * **「物以類聚」的彈性**：KNN 沒有公式限制。你看畫面下方，紫色區域像液體一樣滲透進去，包圍住了那些低票價的死亡群體。
    * **捕捉局部特徵**：它成功捕捉到了資料中的「小聚落」。這代表 KNN 能適應資料的真實形狀，而不是強迫資料去適應一條直線。

---

## 3. 深度分析：尋找最佳鄰居 (圖二解析)

既然 KNN 是看鄰居，那「看幾個鄰居」就成了關鍵。我們測試了 K 從 1 到 40 的結果。

![Accuracy and Confusion Matrix](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-2.jpg?raw=true)

### 左圖：K 值調參 (Accuracy vs. K Value)
* **功能**：這張折線圖顯示了 **K 值 (橫軸)** 與 **預測準確率 (縱軸)** 的關係。紅色虛線標示出了最佳的 K 值 (K=17)。
* **意義**：
    * **過度擬合 (Overfitting)**：如果 K 太小 (如 K=1)，準確率反而低。因為你太容易被單一一個「怪鄰居」誤導（雜訊）。
    * **低度擬合 (Underfitting)**：如果 K 太大 (如 K=40)，準確率也下降。因為你參考太多人，結果變成「大眾臉」，失去了局部特徵。
    * **黃金交叉點**：我們發現 **K=17** 是甜蜜點 (Sweet Spot)，此時模型既不盲從也不固執，準確率達到最高 (約 81.6%)。

### 右圖：混淆矩陣 (Confusion Matrix)
* **功能**：這是 K=17 時的最終成績單。
* **意義**：
    * **對角線 (深色區)**：預測正確的數量。**93** 人正確預測死亡，**53** 人正確預測生存。
    * **非對角線 (淺色區)**：犯錯的地方。
        * **21 (False Negative)**：這些人其實活著，但因為他們的鄰居大多都死了（可能是三等艙的倖存者），導致模型誤判他們會死。這顯示了 KNN 在處理「少數群體」時的弱點。

### 準確率
![Accuracy](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-3.jpg?raw=true)

* **Logistic Regression Accuracy**: 0.8101
* **KNN (K=17) Accuracy**: 0.8156

---
## 4. 戰略總結：火箭與燃料 (Andrew Ng 的觀點)

最後，讓我們引用 AI 大師 **吳恩達 (Andrew Ng)** 的經典圖表，來重新審視我們這幾天學到的模型。
這張圖解釋了為什麼我們需要學習不同的演算法：



## 4. 戰略總結：火箭與燃料 (Andrew Ng 的觀點)

最後，讓我們引用 AI 大師 **吳恩達 (Andrew Ng)** 的經典圖表，來重新審視我們這幾天學到的模型，並將今天學到的 $K$ 值概念放入這張圖中。



### 4.1 火箭 (模型) 與 燃料 (數據)
吳恩達將 **模型 (Model)** 比喻為 **火箭引擎**，將 **數據 (Data)** 比喻為 **燃料**。

![Rocket](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-4.jpg?raw=true)

* **簡單模型 (Small Rocket) / 高 K 值：**
    * 就像**小引擎**。
    * **特性**：結構簡單，參數少（或像 KNN 參考很多人意見，變得很平滑）。
    * **極限**：給它再多燃料，它也跑不快。你會發現紅線最左邊雖然上升，但很快就平掉 (Plateau) 了。

* **複雜模型 (Big Rocket) / 低 K 值：**
    * 就像**巨大的火箭引擎**。
    * **特性**：結構複雜，能捕捉極其細微的特徵（或像 KNN 只看最近的一個人）。
    * **潛力**：只要給它海量的數據（燃料），它的效能會無限上升，這就是深度學習 (Deep Learning) 的強項。

### 4.2 K 值的兩難：過擬合與低度擬合的拉鋸戰
在 KNN 的世界裡，調整 $K$ 值就是在調整「引擎的大小」。

#### **情境一：火箭太大，燃料不夠 (Overfitting / 過擬合)**
* **設定**：**$K=1$ (只看最近的一個鄰居)**
* **意義**：這相當於一個**超級複雜的模型 (Big Rocket)**。
* **現象**：
    * 模型對每一個數據點都極度敏感。
    * 如果有雜訊（例如標記錯誤的點），模型會立刻跟著錯。
    * **結果**：就像把法拉利引擎裝在腳踏車上。在訓練集上表現完美 (100%)，但在未知的測試集上會摔得粉身碎骨。**這就是 High Variance。**

#### **情境二：火箭太小，推不動 (Underfitting / 低度擬合)**
* **設定**：**$K=100$ (參考身邊 100 個人的意見)**
* **意義**：這相當於一個**超級簡單的模型 (Small Rocket)**。
* **現象**：
    * 模型變得太過平滑，變成「大眾臉」。
    * 它忽略了局部的細節，只看整體平均值。
    * **結果**：就像用除草機引擎去推太空梭。無論有多少數據，它都只能給出一個很平庸的預測，無法達到高分。**這就是 High Bias。**

### 4.3 尋找甜蜜點 (The Sweet Spot)
我們在圖表中看到的 **$K=17$**，就是我們找到的**最佳引擎大小**。
它既不會因為 $K$ 太小而過度敏感 (Overfitting)，也不會因為 $K$ 太大而過度平庸 (Underfitting)。它剛好能消耗我們手上的燃料 (Titanic 891筆數據)，跑出最遠的距離 (82% 準確率)。

### 4.4 KNN 在哪裡？
KNN 處於一個有趣的中間位置。

* 它不需要訓練（沒有引擎結構），它完全依賴數據量。
* **數據越多，KNN 越準**（因為鄰居越密集，估計越準）。
* 但它的代價是 **計算速度**。當數據量達到「火箭燃料」等級時，KNN 會因為計算太慢而被淘汰，這時我們就需要換成 **深度學習**。

---

## 5. 總結 (Conclusion)

今天我們見證了 **非參數模型 (KNN)** 與 **參數模型 (Logistic Regression)** 的對決。

* **Logistic Regression** 是理性的：它相信世界有簡單的規則（直線）。
* **KNN** 是經驗的：它相信世界是複雜的，直接參考身邊的案例（鄰居）。

在鐵達尼號這個題目上，KNN 透過捕捉不規則形狀，準確率稍微勝過了邏輯回歸。但 KNN 有個致命傷：**計算量太大**。每次預測都要把所有人抓出來算一次距離，這在大數據時代是不可接受的。

### Next Step:
有沒有一種方法，可以擁有 KNN 的「非線性超強分類能力」，但又像邏輯回歸一樣「算出一條線就好」(不用每次都查鄰居)？

有的！那就是機器學習中期的霸主 —— **Day 06 支援向量機 (SVM)**。我們要學習如何將資料投影到高維空間，用「核函數 (Kernel Trick)」使出魔法，在四維空間切出一條完美的界線！