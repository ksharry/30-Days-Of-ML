# Day 05：K-近鄰演算法 (KNN) —— 命運取決於你的鄰居

## 0. 歷史小故事：來自空軍基地的反叛
在 1951 年，德州蘭道夫空軍基地 (US Air Force School of Aviation Medicine) 發布了一份技術報告，作者是兩位統計學家 **伊芙琳·費克斯 (Evelyn Fix)** 與 **約瑟夫·霍奇斯 (Joseph Hodges)**。

當時的統計學界（如我們昨天學的邏輯回歸）癡迷於尋找完美的「數學公式」來描述數據分佈。但費克斯和霍奇斯提出了一個離經叛道的想法：
> 「如果我們完全不假設數據符合任何公式呢？如果我們直接讓數據自己說話呢？」

這就是 **非參數統計 (Non-parametric Statistics)** 的起源。他們提出的方法非常直觀：如果你想知道一個新數據點屬於哪一類，只要看看它附近最近的幾個點是什麼類別就好。這份報告奠定了 **K-Nearest Neighbors (KNN)** 的基礎，將機器學習從死板的公式中解放出來，進入了「幾何直覺」的時代。

---

## 1. 資料介紹與幾何視角
在 Day 04，我們用邏輯回歸像個嚴肅的法官，試圖畫出一條筆直的「紅線」將乘客分為生存區與死亡區。但如果這條界線不是直的怎麼辦？

今天，我們拋棄複雜的數學公式，使用 KNN 演算法：**「近朱者赤，近墨者黑」。**

### 1.1 Python 程式碼實作
完整程式連結-[KNN_Titanic.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/KNN_Titanic.py)
*(程式碼包含標準化處理與視覺化繪圖)*

---

## 2. 實驗結果：形狀的戰爭 (圖一解析)

我們將模型簡化，只使用 **Age (年齡)** 與 **Fare (票價)** 兩個特徵，畫出了這張對比圖。這張圖展示了兩種截然不同的世界觀。

![Decision Boundary Comparison](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-1.jpg?raw=true)

### 左圖：邏輯回歸 (Logistic Regression)
* **功能**：這是一條 **直線決策邊界 (Linear Decision Boundary)**。它將畫面一分為二，藍色區域預測死亡，白色區域預測生存。
* **意義**：
    * **「線性假設」的僵化**：邏輯回歸認為，票價越高、年齡越小，生存率就「線性地」越高。
    * **無法處理特例**：請注意畫面中間有些藍色點（倖存者）混在白色點（死亡者）中間，邏輯回歸的那條直線完全切不開它們，只能選擇無視。這就是「低度擬合 (Underfitting)」的徵兆。

### 右圖：K-近鄰演算法 (KNN)
* **功能**：這是不規則的、破碎的 **非線性決策邊界 (Non-Linear Decision Boundary)**。紫色區域預測死亡，白色區域預測生存。
* **意義**：
    * **「物以類聚」的彈性**：KNN 沒有公式限制。你看畫面下方，紫色區域像液體一樣滲透進去，包圍住了那些低票價的死亡群體。
    * **捕捉局部特徵**：它成功捕捉到了資料中的「小聚落」。這代表 KNN 能適應資料的真實形狀，而不是強迫資料去適應一條直線。

---

## 3. 深度分析：尋找最佳鄰居 (圖二解析)

既然 KNN 是看鄰居，那「看幾個鄰居」就成了關鍵。我們測試了 K 從 1 到 40 的結果。

![Accuracy and Confusion Matrix](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-2.jpg?raw=true)

### 左圖：K 值調參 (Accuracy vs. K Value)
* **功能**：這張折線圖顯示了 **K 值 (橫軸)** 與 **預測準確率 (縱軸)** 的關係。紅色虛線標示出了最佳的 K 值 (K=17)。
* **意義**：
    * **過度擬合 (Overfitting)**：如果 K 太小 (如 K=1)，準確率反而低。因為你太容易被單一一個「怪鄰居」誤導（雜訊）。
    * **低度擬合 (Underfitting)**：如果 K 太大 (如 K=40)，準確率也下降。因為你參考太多人，結果變成「大眾臉」，失去了局部特徵。
    * **黃金交叉點**：我們發現 **K=17** 是甜蜜點 (Sweet Spot)，此時模型既不盲從也不固執，準確率達到最高 (約 81.6%)。

### 右圖：混淆矩陣 (Confusion Matrix)
* **功能**：這是 K=17 時的最終成績單。
* **意義**：
    * **對角線 (深色區)**：預測正確的數量。**93** 人正確預測死亡，**53** 人正確預測生存。
    * **非對角線 (淺色區)**：犯錯的地方。
        * **21 (False Negative)**：這些人其實活著，但因為他們的鄰居大多都死了（可能是三等艙的倖存者），導致模型誤判他們會死。這顯示了 KNN 在處理「少數群體」時的弱點。

### 準確率
![Accuracy](https://github.com/ksharry/30-Days-Of-ML/blob/main/day5/pic/5-3.jpg?raw=true)

* **Logistic Regression Accuracy**: 0.8101
* **KNN (K=17) Accuracy**: 0.8156

---
## 4. 戰略總結：火箭發射之旅 (模型訓練 SOP)
最後，讓我們引用 AI 大師 **吳恩達 (Andrew Ng)** 的經典圖表，來重新審視我們這幾天學到的模型，並將今天學到的 $K$ 值概念放入這張圖中。

![Rocket](https://github.com/ksharry/30-Days-Of-ML/blob/main/day2/pic/2-6.jpg?raw=true)

這張圖完美地將機器學習模型訓練、參數調整的過程，用「火箭發射」的比喻具象化了。我們可以把這個流程圖看作是訓練一個成功 AI 模型的標準操作程序 (SOP)。

讓我們結合這張圖與 Day 05 學到的知識（特別是 KNN 的 $K$ 值），重新解釋第四章的概念：

**核心流程：**

1.  **開始訓練 (Start Training)**：
    這是起點。我們會選擇一個初始模型（例如：$K=1$ 的 KNN，或者一個複雜的深度學習網絡）和一組參數，然後用訓練資料開始訓練。這就像是在火箭發射台上準備點火。

2.  **決策點 1：訓練集表現好嗎？ (Training Set Performance?)**
    * 這個步驟是在檢查我們的火箭引擎是否足夠強大，能否克服地心引力（訓練資料的複雜度）順利升空。這是在問：「我們的火箭引擎（模型複雜度）是否足以推動我們現有的燃料（訓練資料）？」
    * **否 (No) -> 欠擬合 (Underfitting)**：
        * **圖示解讀**：火箭在發射台掙扎，推力不足，無法升空。這代表我們的模型太簡單，連訓練資料都學不會。這就像是一個小引擎試圖推動大量的燃料，根本推不動。
        * **KNN 例子**：這就像我們選擇了 **$K=100$ (參考太多人)**。模型變得太過平滑，就像一個「大眾臉」，忽略了局部的細節，只看整體平均值。這就是 **High Bias**。
        * **行動 (Action)**：箭頭指向「調整參數：增加複雜度」。例如，我們可以**減小 $K$ 值**，讓模型能捕捉到更細微的特徵，或者換一個更複雜的模型（更大的引擎、更多層數的神經網路），讓火箭有足夠的推力。

3.  **決策點 2：測試/驗證集表現好嗎？ (Test/Validation Set Performance?)**
    * 如果訓練集表現很好（火箭成功升空了），接下來我們要檢查它是否飛得穩，能否適應未知的環境（測試資料）。這是在問：「我們的火箭（模型）在面對未知的太空環境（新資料）時，是否還能保持穩定？」
    * **否 (No) -> 過擬合 (Overfitting)**：
        * **圖示解讀**：火箭飛得很高，但失去控制，甚至燒毀。這代表模型對訓練資料「死記硬背」，導致在面對新資料時完全不知所措，變異度 (Variance) 太高。這就像把法拉利引擎裝在腳踏車上，速度雖快但極不穩定。
        * **KNN 例子**：這就像我們選擇了 **$K=1$ (只參考一個人)**。模型對每一個數據點都極度敏感，如果有雜訊（例如標記錯誤的點），模型會立刻跟著錯。這就是 **High Variance**。
        * **行動 (Action)**：箭頭指向「調整參數：限制複雜度或增加正則化」。例如，我們可以**增大 $K$ 值**（參考更多人意見來平均風險），或者在損失函數中加入 Day 03 學到的 L1/L2 正則化項，為失控的火箭加上穩定系統。給火箭更多燃料（更多資料）也是一個好方法，因為這能讓模型學習到更普遍的規則，而不是只記住訓練資料的雜訊。

4.  **終點：表現良好 (Good Performance)**
    * **是 (Yes)**：如果模型在訓練集和測試集上的表現都讓人滿意，這就代表我們找到了「甜蜜點 (Sweet Spot)」。
    * **圖示解讀**：火箭成功進入預定軌道，穩定運行。這就是我們在 Day 05 找到的 **$K=17$**，它達到了 Bias 與 Variance 的完美平衡。
    * **完成！ (Done!)**：我們的模型訓練完成了！

**總結來說，機器學習就是一場不斷在「欠擬合（推力不足）」與「過擬合（推力過大）」之間尋找平衡的戰爭。**

---

## 5. 總結 (Conclusion)

今天我們見證了 **非參數模型 (KNN)** 與 **參數模型 (Logistic Regression)** 的對決。

* **Logistic Regression** 是理性的：它相信世界有簡單的規則（直線）。
* **KNN** 是經驗的：它相信世界是複雜的，直接參考身邊的案例（鄰居）。

在鐵達尼號這個題目上，KNN 透過捕捉不規則形狀，準確率稍微勝過了邏輯回歸。但 KNN 有個致命傷：**計算量太大**。每次預測都要把所有人抓出來算一次距離，這在大數據時代是不可接受的。

### Next Step:
有沒有一種方法，可以擁有 KNN 的「非線性超強分類能力」，但又像邏輯回歸一樣「算出一條線就好」(不用每次都查鄰居)？

有的！那就是機器學習中期的霸主 —— **Day 06 支援向量機 (SVM)**。我們要學習如何將資料投影到高維空間，用「核函數 (Kernel Trick)」使出魔法，在四維空間切出一條完美的界線！