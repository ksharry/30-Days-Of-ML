# 30-Days-Of-ML

2. py -3.10 California_Housing_Prediction.py
3. py -3.10 Regularization_Demo.py
4. py -3.10 Logistic_Titanic.py
5. py -3.10 NaiveBayes_Spam.py

# 30 天 AI/ML 實戰挑戰：從入門到落地 (Story-Driven Edition)

這份課表採用 **「故事線 (Storyline)」** 設計，透過共用經典資料集（如鐵達尼號、員工離職數據），深入比較不同演算法在解決同一問題時的差異與進化。

---

## 第一週：機器學習基石 (Foundations & Supervised Learning)

**故事線 A：房價預測 (回歸)** — *從簡單線性模型到修正過擬合*
**故事線 B：鐵達尼號生存預測 (分類)** — *同一個分類問題，三種幾何視角的解法*

| Day | 主題 | 共用題目 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **01** | **AI 概論與地圖** | *(無代碼)* | **起點**：AI vs ML vs DL 的關係、弱人工智慧 (ANI) 到強人工智慧 (AGI)、學習路徑規劃。 |
| **02** | 線性回歸 (Linear) | **加州房價** | **建立基準**：最簡單的模型。了解 MSE Loss，畫出第一條預測線。 |
| **03** | 正則化 (Ridge/Lasso) | **加州房價** | **修正錯誤**：模型太複雜導致過擬合 (Overfitting)？用 L1/L2 懲罰項把權重壓回來。 |
| **04** | 邏輯回歸 (Logistic) | **鐵達尼號** | **分類起手式**：預測「生存機率」。學習 Sigmoid 函數與 Decision Boundary。 |
| **05** | K-近鄰 (KNN) | **鐵達尼號** | **幾何視角**：不畫線，而是「看鄰居」。比較 KNN 與邏輯回歸在準確度上的差異。 |
| **06** | 支援向量機 (SVM) | **鐵達尼號** | **尋找完美邊界**：試圖在生存與死亡之間畫出一條「最寬的馬路 (Max Margin)」。 |
| **07** | 貝氏分類 (Naive Bayes) | *垃圾郵件* | **機率視角**：(插曲) 處理文字特徵。利用貝氏定理實作簡單的 Spam Filter。 |

---

## 第二週：表格資料王者與非監督學習 (Trees & Unsupervised)

**故事線 C：員工離職預測 (HR Analytics)** — *Kaggle 競賽級別的實戰*
這週專注於單一資料集，從預測「誰會走」到分析「為什麼走」，再到「員工分群」。

| Day | 主題 | 共用題目 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **08** | 決策樹 (Decision Tree) | **員工離職** | **白箱模型**：畫出決策樹圖，找出離職第一關鍵因素（如：滿意度 < 0.5）。 |
| **09** | 隨機森林 (Random Forest) | **員工離職** | **集成力量 (Bagging)**：一棵樹容易誤判，種一片森林來投票。觀察 Feature Importance 變化。 |
| **10** | XGBoost | **員工離職** | **提升力量 (Boosting)**：不再只是投票，而是針對「上一棵樹做錯的題目」加強訓練。 |
| **11** | LightGBM | **員工離職** | **速度極限**：假設員工變 100 萬人，如何用更聰明的生長策略 (Leaf-wise) 加速訓練？ |
| **12** | K-Means 分群 | **員工離職** | **非監督發現**：拿掉標籤。能不能自動把員工分成「高薪懶散族」或「低薪過勞族」？ |
| **13** | 主成分分析 (PCA) | **員工離職** | **降維視覺化**：HR 資料有 20 個維度畫不出來？壓成 2D 散佈圖，看離職群體是否聚在一起。 |
| **14** | 密度分群 (DBSCAN) | *月亮形狀* | **幾何分群**：(特殊幾何資料) 解決 K-Means 無法處理圓形/不規則形狀分群的弱點。 |

---

## 第三週：深度學習與電腦視覺 (Deep Learning & Vision)

**故事線 D：Fashion-MNIST (衣物辨識)** — *從 MLP 到 CNN 的進化*
**故事線 E：CIFAR-10 (彩色圖片)** — *挑戰更難的視覺任務*

| Day | 主題 | 共用題目 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **15** | 多層感知器 (MLP) | **Fashion-MNIST** | **神經網路入門**：把圖片拉平 (Flatten)，用全連接層硬解，觀察其極限。 |
| **16** | 優化器與 Loss | **Fashion-MNIST** | **調參實驗室**：架構不變，只換 SGD vs Adam，看誰收斂得快？了解 CrossEntropy。 |
| **17** | CNN 基礎 (卷積) | **Fashion-MNIST** | **空間感知**：不再拉平圖片。引入 Conv2D 與 Pooling，參數變少但分數變高。 |
| **18** | 經典架構 VGG16 | **CIFAR-10** | **深度堆疊**：換成彩色圖片。學習如何像堆積木一樣堆疊深層網路。 |
| **19** | 遷移學習 (Transfer) | **CIFAR-10** | **巨人肩膀**：自己練太慢？載入 ImageNet 預訓練權重，只微調最後一層 (Fine-tuning)。 |
| **20** | 資料增強 (Augmentation)| **CIFAR-10** | **以量取勝**：圖片太少？透過旋轉、翻轉、縮放「偽造」數據，提升模型強健度。 |
| **21** | 殘差網路 (ResNet) | *自選圖片* | **突破深度極限**：解決梯度消失問題，實作 Skip Connection 機制。 |

---

## 第四週：序列模型、生成式 AI 與總結 (NLP & Future)

**故事線 F：IMDB 電影評論 (情感分析)** — *從文字到語意*

| Day | 主題 | 共用題目 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **22** | 循環神經網路 (RNN) | **IMDB 評論** | **序列觀念**：處理文字順序。把句子轉成向量，訓練一個正負評判斷器。 |
| **23** | LSTM / GRU | **IMDB 評論** | **長期記憶**：解決 RNN 記不住長句子的問題。比較 LSTM 與 GRU 的效能。 |
| **24** | 詞嵌入 (Word2Vec) | *文字語料* | **語意空間**：讓電腦理解 `King - Man + Woman = Queen` 的向量魔法。 |
| **25** | Transformer 概念 | *翻譯任務* | **注意力機制**：Attention Is All You Need。BERT 與 GPT 的核心基石。 |
| **26** | 自動編碼器 (Autoencoder)| *圖片去噪* | **壓縮與還原**：非監督學習。輸入模糊圖片，訓練 AI 輸出清晰圖片。 |
| **27** | 生成對抗網路 (GAN) | *手寫數字生成* | **左右互搏**：Generator (造假) vs Discriminator (鑑識)，從無到有生成圖片。 |
| **28** | 強化學習 (RL) | *走迷宮/CartPole*| **獎勵機制**：Q-Learning 入門。讓 Agent 在遊戲環境中透過試誤學習策略。 |
| **29** | 模型部署 (Deployment) | *Web App* | **落地應用**：使用 `joblib` 儲存模型，並用 Streamlit 做成一個網頁給朋友玩。 |
| **30** | **AI 總結與未來展望** | *(無代碼)* | **終點即起點**：回顧這 29 天的技能樹。探討 MLOps、倫理、以及下一步如何深入 (如 LLM)。 |