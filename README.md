# 30-Days-Of-ML

2. py -3.10 California_Housing_Prediction.py
3. py -3.10 Regularization_Demo.py
4. py -3.10 Logistic_Titanic.py
5. py -3.10 NaiveBayes_Spam.py

# 30 天 AI/ML 實戰挑戰：從入門到落地 (Story-Driven Edition)

這份課表採用 **「故事線 (Storyline)」** 設計，透過共用經典資料集（如鐵達尼號、員工離職數據），深入比較不同演算法在解決同一問題時的差異與進化。

---

## 主題一：數值與類別的幾何基礎 (Foundations)
**核心資料集：加州房價 (數值) & 鐵達尼號 (類別)**
*故事線：從畫一條線預測房價，到畫一條邊界決定生死。*

| Day | 演算法/主題 | 資料集 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **01** | AI 概論與地圖 | *(無)* | **起點**：AI vs ML vs DL 的關係、環境建置 (Anaconda/Colab)。 |
| **02** | 線性回歸 (Linear) | **加州房價** | **預測數值**：最簡單的模型。了解 MSE Loss，畫出第一條預測線。 |
| **03** | 正則化 (Lasso/Ridge)| **加州房價** | **修正過擬合**：模型太複雜怎麼辦？用 L1/L2 懲罰項把權重壓回來。 |
| **04** | 邏輯回歸 (Logistic) | **鐵達尼號** | **分類起手式**：預測「生存機率」。學習 Sigmoid 函數與 Decision Boundary。 |
| **05** | K-近鄰 (KNN) | **鐵達尼號** | **幾何視角**：不畫線，而是「看鄰居」。比較 KNN 與邏輯回歸的差異。 |
| **06** | 支援向量機 (SVM) | **鐵達尼號** | **幾何極限**：試圖畫出「最寬的馬路」。體會幾何分割在處理複雜數據時的黑箱與算力瓶頸。 |

---

## 主題二：結構化資料的進階與偵探 (Tabular Advanced)
**核心資料集：員工離職 (HR) & 晶圓瑕疵 (Wafer)**
*故事線：追求 Kaggle 級別的準確度，並學會「解釋」模型與找出「異常」。*

| Day | 演算法/主題 | 資料集 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **07** | 決策樹 (Decision Tree)| **員工離職** | **白箱模型**：幾何模型難解釋？畫出決策樹，找出離職第一關鍵因素（如：滿意度 < 0.5）。 |
| **08** | 隨機森林 (Random Forest)| **員工離職** | **集成投票**：一棵樹容易誤判，種一片森林來投票 (Bagging)，大幅提升穩健度。 |
| **09** | XGBoost / LightGBM | **員工離職** | **接力修正**：不再只是投票，而是針對上一棵樹「做錯的題目」加強訓練 (Boosting)。業界競賽王者。 |
| **10** | **可解釋 AI (SHAP)** | **員工離職** | **解開黑箱**：XGBoost 雖然準但看不懂？引入 **SHAP Values**，視覺化告訴老闆「為什麼」模型判定這位員工會走。 |
| **11** | 主成分分析 (PCA) | **員工離職** | **降維視覺化**：資料有 20 個維度畫不出來？壓成 2D 散佈圖，看離職群體是否聚在一起。 |
| **12** | K-Means 分群 | **員工離職** | **自動分群**：拿掉標籤。能不能自動把員工分成「高薪懶散族」或「低薪過勞族」？ |
| **13** | 密度分群 (DBSCAN) | **晶圓圖譜** | **形狀分群**：(彎月形資料) K-Means 切不了彎月形。DBSCAN 用密度概念成功分離複雜幾何圖形。 |
| **14** | 孤立森林 (iForest) | **晶圓圖譜** | **異常偵測**：(雜訊資料) 不看形狀，專抓雜點。利用隨機切割快速揪出產線上的異常瑕疵。 |

---

## 主題三：黑白影像的辨識與生成 (Monochrome Vision)
**核心資料集：Fashion-MNIST (衣物圖片)**
*故事線：從「看懂」一張圖，到「修復」一張圖，最後「創造」一張圖。*

| Day | 演算法/主題 | 資料集 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **15** | 多層感知器 (MLP) | **Fashion-MNIST** | **破壞結構**：把圖片拉平成一維向量硬解。雖然能跑，但失去了空間資訊。 |
| **16** | 優化器與 Loss | **Fashion-MNIST** | **調參實驗室**：架構不變，比較 SGD vs Adam 的收斂速度與 CrossEntropy 原理。 |
| **17** | CNN 基礎 (卷積) | **Fashion-MNIST** | **空間感知**：引入 Conv2D，終於能理解「形狀」與「紋理」。參數變少，準確度變高。 |
| **18** | 自動編碼器 (Autoencoder)| **Fashion-MNIST** | **修復大師**：(非監督) 練習輸入「有雜訊/模糊」的衣服圖，訓練 AI 輸出「清晰」的原圖。 |
| **19** | 生成對抗網路 (GAN) | **Fashion-MNIST** | **AI 設計師**：(生成式) Generator 負責憑空畫出衣服，Discriminator 負責鑑識，讓 AI 學會創造新款式。 |

---

## 主題四：彩色影像與深度極限 (Deep Color Vision)
**核心資料集：CIFAR-10 (彩色圖片)**
*故事線：面對真實世界的彩色照片，我們需要更深、更強的架構。*

| Day | 演算法/主題 | 資料集 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **20** | 經典架構 VGG16 | **CIFAR-10** | **深度堆疊**：處理 RGB 三原色。學習像堆積木一樣堆疊深層網路來提取特徵。 |
| **21** | 資料增強 (Augmentation)| **CIFAR-10** | **以量取勝**：資料不夠？透過旋轉、翻轉、縮放「偽造」數據，防止模型死背答案。 |
| **22** | 遷移學習 (ResNet) | **CIFAR-10** | **巨人肩膀**：引入殘差連接 (Skip Connection) 與 ImageNet 預訓練權重，解決梯度消失並大幅加速訓練。 |

---

## 主題五：自然語言處理與落地 (NLP & Deployment)
**核心資料集：IMDB 電影評論 (文字)**
*故事線：從統計關鍵字，到理解上下文，最後做成 App 給朋友玩。*

| Day | 演算法/主題 | 資料集 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **23** | 貝氏分類 (Naive Bayes)| **IMDB 評論** | **NLP 基準線**：使用詞袋模型 (BoW) + 機率統計。速度快，作為挑戰 Deep Learning 的門檻。 |
| **24** | 循環神經網路 (RNN) | **IMDB 評論** | **序列觀念**：電腦開始讀懂「順序」。處理 "not good" 這種前後文依賴的語句。 |
| **25** | 長短期記憶 (LSTM) | **IMDB 評論** | **長期記憶**：解決 RNN 金魚腦（記不住長文）的問題，捕捉整段影評的情緒。 |
| **26** | 詞嵌入 (Word2Vec) | **IMDB 評論** | **語意空間**：自己訓練 Word Vectors。發現 "bad" 和 "awful" 在向量空間中靠得很近。 |
| **27** | Transformer (BERT) | **IMDB 評論** | **注意力機制**：(概念與微調) 簡單介紹 Attention，並嘗試 Fine-tune 一個小型的 BERT 模型來做分類。 |
| **28** | 模型部署 (Deployment) | **IMDB 評論** | **落地應用**：將 Day 25 或 27 訓練好的模型存檔。使用 **Streamlit** 架設一個網頁，即時分析輸入文字的情緒。 |

---

## 主題六：強化學習與總結 (The Frontier)
**核心資料集：OpenAI Gym (遊戲環境)**

| Day | 演算法/主題 | 資料集 | 學習重點與「故事劇情」 |
| :--- | :--- | :--- | :--- |
| **29** | 強化學習 (RL) | *CartPole* | **自主學習**：(番外篇) 唯一的動態環境。讓 Agent 透過 Q-Learning 試誤，學會玩倒立擺遊戲。 |
| **30** | AI 總結與未來 | *(無)* | **終點即起點**：回顧這 29 天的技能樹。探討 MLOps、AI 倫理、以及 LLM (Large Language Models) 的學習方向。 |