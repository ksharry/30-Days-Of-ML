# Day 07：貝氏分類器 (Naive Bayes) —— 來自 18 世紀的機率智慧

## 0. 歷史小故事：被遺忘的手稿

我們今天使用的高科技垃圾郵件過濾器，其核心靈魂竟源自於 18 世紀一位低調的英國牧師與業餘數學家——**托馬斯·貝葉斯 (Thomas Bayes)**。

貝葉斯生前並沒有發表他最重要的成果。直到他去世後，他的朋友理查·普萊斯 (Richard Price) 在整理遺物時，才發現了那份探討「機會論」的手稿。普萊斯慧眼識珠，將其整理發表，世人才得以知曉這個後來改變了統計學、甚至引發了 AI 革命的定理。

貝葉斯的核心思想非常簡單卻深刻：**「我們對世界的看法（信念），應該隨著新的證據出現而更新。」**

---

## 1. 資料介紹

### 1.1 資料集介紹 (Data Dictionary)

本實作使用機器學習領域最經典的 **UCI SMS Spam Collection** 資料集。這是一份真實的簡訊數據，包含了 5,574 條英文簡訊，被標記為正常郵件或垃圾郵件。

資料來源：[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)

| 欄位名稱 (Column) | 說明 (Description) | 備註 (Key) |
| :--- | :--- | :--- |
| **Label** | 郵件類別 | **ham** = 正常簡訊 (Normal)<br>**spam** = 垃圾簡訊 (Spam) |
| **Text** | 簡訊原始內容 | 未經處理的英文原始文字串 (Raw Text) |

### 1.2 資料範例
這份資料沒有像鐵達尼號那樣的數值特徵（年齡、票價），只有赤裸裸的文字：

* **Ham**: "Ok lar... Joking wif u oni..."
* **Spam**: "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005."

這引出了一個巨大的挑戰：**電腦看不懂文字，它只看得懂數字。** 我們該怎麼把這些英文單字餵給模型吃？

---

## 2. (插曲) 處理文字特徵：詞袋模型

為了讓電腦「讀懂」文字，我們採用一種最簡單粗暴的方法：**詞袋模型 (Bag-of-Words, BoW)**。

想像你有一個巨大的袋子，把所有簡訊裡的字都剪下來丟進去，搖一搖，忽略它們出現的順序，只在乎**「什麼詞出現了」**以及**「出現了幾次」**。

Let's visualize it:

* **簡訊 A**: "Win cash now"
* **簡訊 B**: "Call now"

轉換成矩陣就會變成這樣：

| | "Win" | "Cash" | "Now" | "Call" |
| :--- | :---: | :---: | :---: | :---: |
| **簡訊 A** | **1** | **1** | **1** | 0 |
| **簡訊 B** | 0 | 0 | **1** | **1** |

現在，每一則簡訊都變成了一個由數字組成的向量（一行數據），電腦終於可以處理了！這在 Python 中由 `CountVectorizer` 來實現。

---

## 3. 理論基礎：簡單到「天真」的假設

貝氏分類器的核心當然是**貝氏定理 (Bayes' Theorem)**。

別怕數學公式，我們用垃圾郵件的例子來看，它其實超直觀：

$$P(\text{垃圾} | \text{文字}) = \frac{P(\text{文字} | \text{垃圾}) \times P(\text{垃圾})}{P(\text{文字})}$$

我們想求的是左邊：**「當我看到這則簡訊的內容時，它是垃圾信的機率有多少？」** (後驗機率)

為了算出這個，我們需要知道右邊三件事：
1.  **$P(\text{垃圾})$**：**先驗機率 (Prior)**。不管內容是什麼，隨便抓一則簡訊是垃圾信的機率有多大？（以 UCI 資料集來說，約 13.4% 是垃圾信）。
2.  **$P(\text{文字} | \text{垃圾})$**：**似然性 (Likelihood)**。如果已知這是一封垃圾信，出現這些文字（例如 "Free", "Win"）的機率有多大？這是模型要從訓練資料中學得的關鍵。
3.  **$P(\text{文字})$**：標準化常數。通常在比較分類時可以忽略。

### 為什麼叫「天真 (Naive)」？

要計算一整句話出現的機率非常困難。為了簡化計算，貝氏分類器做了一個超級強大但**非常不切實際**的假設：

> **「天真」假設：簡訊裡的每一個詞都是相互獨立的，互不影響。**

也就是說，它認為 "Win" 出現的機率，跟後面有沒有接 "Cash" 完全沒關係。這當然是錯的！但在實務上，**樸素貝氏分類器 (Naive Bayes Classifier) 的效果竟然出奇地好**，而且計算速度飛快！

### 3.1 Python 程式碼實作
完整程式連結：[Day07_NaiveBayes_UCI.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/Day07_NaiveBayes_UCI.py)
*(程式碼包含自動下載 UCI 資料集與文字前處理)*

---

## 4. 實驗結果：透視貝氏的腦袋

我們使用真實的 UCI 資料集訓練模型，結果通常會讓人驚艷。

![Naive Bayes Results](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/pic/image_f7c8e2.png?raw=true)
*(示意圖，實際執行結果可能會有所不同)*

### 4.1 準確率 (Accuracy)
在 UCI 資料集上，多項式貝氏分類器 (MultinomialNB) 的準確率通常能達到 **98% 以上**。這證明了對於「簡訊」這種短文本，關鍵字出現的頻率具有極強的決定性。

### 4.2 關鍵詞分析 (Top "Spammy" vs. "Hammy" Words)
這張圖是貝氏模型最棒的地方——**高度的可解釋性**。我們可以計算每個單字的「垃圾度得分」(Spamminess Score)。

* **紅色長條 (Spam Keywords)**：
    * 模型學到了諸如 **"claim", "prize", "won", "urgent", "free"** 等詞組是非常強烈的垃圾信訊號。
    * 這完全符合我們的直覺！詐騙簡訊通常充滿了誘惑性或急迫性的詞彙。

* **藍色長條 (Ham Keywords)**：
    * 相反地，諸如 **"later", "home", "sorry", "today"** 等生活用語，則是正常簡訊的特徵。

貝氏分類器就是這樣一個單純的模型：它不理解語義，它只是在**比較機率**。如果一封信裡「紅色詞彙」的總機率乘積大於「藍色詞彙」，它就蓋上「垃圾信」的印章。

---

## 5. 總結 (Conclusion)

今天我們跨越了兩個里程碑：處理了**非結構化文字**，並引入了**機率模型**。

* **貝氏分類器**的優點：
    * **速度快**：訓練和預測都非常快，適合即時應用 (Real-time Prediction)。
    * **小數據表現好**：需要的訓練資料相對較少。
    * **可解釋性強**：我們可以知道哪些詞導致了分類結果。
* **缺點**：
    * **忽略語意**：因為「獨立性假設」，它無法理解 "not good" 和 "good" 的差別（除非用 n-gram）。

### 模型的演進圖譜

回顧我們學過的分類模型，可以發現它們看待世界的方式截然不同：

| 模型 | 視角 | 核心思想 |
| :--- | :--- | :--- |
| **邏輯回歸 (Day 04)** | 幾何視角 | 試圖畫一條**直線**邊界。 |
| **KNN (Day 05)** | 幾何視角 | **記憶**資料，看鄰居是誰。 |
| **SVM (Day 06)** | 幾何視角 | 尋找最完美的**寬邊界** (常涉及高維投影)。 |
| **貝氏分類 (Day 07)** | **機率視角** | 計算特徵出現的**條件機率**。 |

### Next Step:
我們已經看過了幾何和機率的視角。接下來，我們要進入一個更符合人類決策邏輯的領域。

想像你在玩「二十個問題」遊戲，你透過一系列「是/否」的問題來猜測答案。這就是我們下一章的主題——**Day 08 決策樹 (Decision Tree)**。我們將看到模型如何像畫流程圖一樣，一步步切分資料，找出生存法則！