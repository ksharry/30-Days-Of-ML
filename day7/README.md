# Day 07：貝氏分類器 (Naive Bayes) —— 來自 18 世紀的機率智慧

## 0. 歷史小故事：被遺忘的手稿

我們今天使用的高科技垃圾郵件過濾器，其核心靈魂竟源自於 18 世紀一位低調的英國牧師與業餘數學家——**托馬斯·貝葉斯 (Thomas Bayes)**。

貝葉斯生前並沒有發表他最重要的成果。直到他去世後，他的朋友理查·普萊斯 (Richard Price) 在整理遺物時，才發現了那份探討「機會論」的手稿。普萊斯慧眼識珠，將其整理發表，世人才得以知曉這個後來改變了統計學、甚至引發了 AI 革命的定理。

貝葉斯的核心思想非常簡單卻深刻：**「我們對世界的看法（信念），應該隨著新的證據出現而更新。」**

---

## 1. 資料介紹

### 1.1 資料集介紹 (Data Dictionary)

本實作使用機器學習領域最經典的 **UCI SMS Spam Collection** 資料集。這是一份真實的簡訊數據，包含了 5,574 條英文簡訊，被標記為正常郵件或垃圾郵件。

資料來源：[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)

| 欄位名稱 (Column) | 說明 (Description) | 備註 (Key) |
| :--- | :--- | :--- |
| **Label** | 郵件類別 | **ham** = 正常簡訊 (Normal)<br>**spam** = 垃圾簡訊 (Spam) |
| **Text** | 簡訊原始內容 | 未經處理的英文原始文字串 (Raw Text) |

### 1.2 資料範例
這份資料沒有像鐵達尼號那樣的數值特徵（年齡、票價），只有赤裸裸的文字：
![data](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/pic/7-1.jpg?raw=true)

這引出了一個巨大的挑戰：**電腦看不懂文字，它只看得懂數字。** 我們該怎麼把這些英文單字餵給模型吃？

---

## 2. (插曲) 處理文字特徵：詞袋模型

為了讓電腦「讀懂」文字，我們採用一種最簡單粗暴的方法：**詞袋模型 (Bag-of-Words, BoW)**。

想像你有一個巨大的袋子，把所有簡訊裡的字都剪下來丟進去，搖一搖，忽略它們出現的順序，只在乎**「什麼詞出現了」**以及**「出現了幾次」**。

Let's visualize it:

* **簡訊 A**: "Win cash now"
* **簡訊 B**: "Call now"

轉換成矩陣就會變成這樣：

| | "Win" | "Cash" | "Now" | "Call" |
| :--- | :---: | :---: | :---: | :---: |
| **簡訊 A** | **1** | **1** | **1** | 0 |
| **簡訊 B** | 0 | 0 | **1** | **1** |

現在，每一則簡訊都變成了一個由數字組成的向量（一行數據），電腦終於可以處理了！這在 Python 中由 `CountVectorizer` 來實現。

---

## 3. 理論基礎：簡單到「天真」的假設

貝氏分類器的核心當然是**貝氏定理 (Bayes' Theorem)**。

別怕數學公式，我們用垃圾郵件的例子來看，它其實超直觀：

$$P(\text{垃圾} | \text{文字}) = \frac{P(\text{文字} | \text{垃圾}) \times P(\text{垃圾})}{P(\text{文字})}$$

我們想求的是左邊：**「當我看到這則簡訊的內容時，它是垃圾信的機率有多少？」** (後驗機率)

為了算出這個，我們需要知道右邊三件事：
1.  **$P(\text{垃圾})$**：**先驗機率 (Prior)**。不管內容是什麼，隨便抓一則簡訊是垃圾信的機率有多大？（以 UCI 資料集來說，約 13.4% 是垃圾信）。
2.  **$P(\text{文字} | \text{垃圾})$**：**似然性 (Likelihood)**。如果已知這是一封垃圾信，出現這些文字（例如 "Free", "Win"）的機率有多大？這是模型要從訓練資料中學得的關鍵。
3.  **$P(\text{文字})$**：標準化常數。通常在比較分類時可以忽略。

### 為什麼叫「天真 (Naive)」？

要計算一整句話出現的機率非常困難。為了簡化計算，貝氏分類器做了一個超級強大但**非常不切實際**的假設：

> **「天真」假設：簡訊裡的每一個詞都是相互獨立的，互不影響。**

也就是說，它認為 "Win" 出現的機率，跟後面有沒有接 "Cash" 完全沒關係。這當然是錯的！但在實務上，**樸素貝氏分類器 (Naive Bayes Classifier) 的效果竟然出奇地好**，而且計算速度飛快！

### 3.1 Python 程式碼實作
完整程式連結：[Day07_NaiveBayes_UCI.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/Day07_NaiveBayes_UCI.py)
*(程式碼包含自動下載 UCI 資料集與文字前處理)*

---

## 4. 實驗結果：透視貝氏的腦袋

我們使用真實的 UCI 資料集訓練模型，結果通常會讓人驚艷。

![Naive Bayes Results](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/pic/7-2.jpg?raw=true)

### 4.1 準確率 (Accuracy)
在 UCI 資料集上，多項式貝氏分類器 (MultinomialNB) 的準確率通常能達到 **98% 以上**。這證明了對於「簡訊」這種短文本，關鍵字出現的頻率具有極強的決定性。

### 4.2 關鍵詞分析 (Top "Spammy" vs. "Hammy" Words)
這張圖是貝氏模型最棒的地方——**高度的可解釋性**。我們可以計算每個單字的「垃圾度得分」(Spamminess Score)。

* **紅色長條 (Spam Keywords)**：
    * 模型學到了諸如 **"claim", "prize", "won", "urgent", "free"** 等詞組是非常強烈的垃圾信訊號。
    * 這完全符合我們的直覺！詐騙簡訊通常充滿了誘惑性或急迫性的詞彙。

* **藍色長條 (Ham Keywords)**：
    * 相反地，諸如 **"later", "home", "sorry", "today"** 等生活用語，則是正常簡訊的特徵。

貝氏分類器就是這樣一個單純的模型：它不理解語義，它只是在**比較機率**。如果一封信裡「紅色詞彙」的總機率乘積大於「藍色詞彙」，它就蓋上「垃圾信」的印章。


## 6. 戰略總結：模型訓練的火箭發射之旅 (貝氏分類版)

最後，我們再次請出這張「火箭發射 SOP」圖。雖然貝氏分類器以「簡單快速」著稱，但它同樣需要經過這趟發射檢測，才能確保在真實世界的垃圾信海中穩定運行。

![Model Training and Tuning Process](https://github.com/ksharry/30-Days-Of-ML/blob/main/day2/pic/2-6.jpg?raw=true)

### 6.1 流程一：詞彙貧乏，無法判斷 (Underfitting 迴圈)

* **設定**：假設我們的「詞袋」設得太小 (例如 `max_features=10`)，只看整封信裡出現頻率最高的 10 個字。
* **第一關：訓練集表現好嗎？**
    * **否 (No)**。
    * **診斷**：**欠擬合 (Underfitting)**。
        * **原因**：模型掌握的資訊太少。如果只看 "the", "a", "is" 這些字，根本無法區分垃圾信。這就像火箭燃料不足（資訊量不足），根本飛不起來。
* **行動 (Action)**：箭頭指向 **「調整參數：增加複雜度」**。
    * **擴充字典**：增加 `max_features` (例如從 10 改成 3000)，讓模型看懂更多關鍵字。
    * **使用 N-gram**：不只看單字 ("Win")，改看詞組 ("Win Cash")，增加特徵的豐富度。

### 6.2 流程二：鑽牛角尖，過度敏感 (Overfitting 迴圈)

* **設定**：假設我們把所有出現過的字都當成特徵 (包含只出現一次的亂碼、時間戳記)，而且將平滑係數 $\alpha$ 設為 0 (不平滑)。
* **第一關：訓練集表現好嗎？**
    * **是 (Yes)**。模型把每一封信裡的每一個獨特字串都背下來了。
* **第二關：測試集表現好嗎？**
    * **否 (No)**。
    * **診斷**：**過擬合 (Overfitting)**。
        * **原因**：模型記住了太多的雜訊。例如某封垃圾信剛好有一個錯字 "Pr1ze"，模型就認定 "Pr1ze" 是絕對的垃圾信特徵。如果測試集裡剛好沒這個字，或者正常信裡不小心打錯字，機率計算就會出錯 (Zero Probability Problem)。
* **行動 (Action)**：箭頭指向 **「調整參數：限制複雜度」**。
    * **啟動平滑 (Smoothing)**：增加 **$\alpha$ (Alpha)** 值。這就像給機率計算加了一點「緩衝」，避免讓單一罕見字詞決定一切。
    * **過濾雜訊**：設定 `min_df` (例如忽略出現少於 5 次的字)，去除那些太過冷門的雜訊特徵。

### 6.3 流程三：完美入軌 (The Sweet Spot)

* **設定**：我們選用了 `max_features=3000` 配合預設的 `alpha=1.0` (Laplace Smoothing)。
* **第一關 & 第二關**：
    * 訓練集與測試集準確率都極高 (UCI 資料集約 **98%**)。
* **結果**：**完成！**
    * 火箭成功入軌。我們的 Spam Filter 既能識別常見的詐騙關鍵字 (Win, Free)，又不會因為一個從沒見過的生字就判斷錯誤，達到了完美的平衡。

---

## 5. 總結 (Conclusion)

今天我們跨越了兩個里程碑：處理了**非結構化文字**，並引入了**機率模型**。

* **貝氏分類器**的優點：
    * **速度快**：訓練和預測都非常快，適合即時應用 (Real-time Prediction)。
    * **小數據表現好**：需要的訓練資料相對較少。
    * **可解釋性強**：我們可以知道哪些詞導致了分類結果。
* **缺點**：
    * **忽略語意**：因為「獨立性假設」，它無法理解 "not good" 和 "good" 的差別（除非用 n-gram）。

### 模型的演進圖譜

回顧我們學過的分類模型，可以發現它們看待世界的方式截然不同：

| 模型 | 視角 | 核心思想 |
| :--- | :--- | :--- |
| **邏輯回歸 (Day 04)** | 幾何視角 | 試圖畫一條**直線**邊界。 |
| **KNN (Day 05)** | 幾何視角 | **記憶**資料，看鄰居是誰。 |
| **SVM (Day 06)** | 幾何視角 | 尋找最完美的**寬邊界** (常涉及高維投影)。 |
| **貝氏分類 (Day 07)** | **機率視角** | 計算特徵出現的**條件機率**。 |

### Next Step:
我們已經看過了幾何和機率的視角。接下來，我們要進入一個更符合人類決策邏輯的領域。

想像你在玩「二十個問題」遊戲，你透過一系列「是/否」的問題來猜測答案。這就是我們下一章的主題——**Day 08 決策樹 (Decision Tree)**。我們將看到模型如何像畫流程圖一樣，一步步切分資料，找出生存法則！