# Day 07：貝氏分類器 (Naive Bayes) —— 來自 18 世紀的機率智慧

## 0. 歷史小故事：被遺忘的手稿

我們今天使用的高科技垃圾郵件過濾器，其核心靈魂竟源自於 18 世紀一位低調的英國牧師與業餘數學家——**托馬斯·貝葉斯 (Thomas Bayes)**。

貝葉斯生前並沒有發表他最重要的成果。直到他去世後，他的朋友理查·普萊斯 (Richard Price) 在整理遺物時，才發現了那份探討「機會論」的手稿。普萊斯慧眼識珠，將其整理發表，世人才得以知曉這個後來改變了統計學、甚至引發了 AI 革命的定理。

貝葉斯的核心思想非常簡單卻深刻：**「我們對世界的看法（信念），應該隨著新的證據出現而更新。」**

這聽起來很像廢話？不，在古典統計學看來，機率是客觀存在的頻率（例如丟硬幣正面就是 50%）。但貝葉斯學派認為，機率可以是主觀的「信念程度」，而這個信念可以透過數據來修正。這就是現代機器學習的基石——**從數據中學習**。

---

## (插曲) 處理文字特徵：電腦如何閱讀？

在進入貝氏定理之前，我們面臨一個巨大的挑戰：今天的題目是「垃圾郵件分類」。

電腦只看得懂數字矩陣（像前幾天的鐵達尼號資料）。如果你給它一句話：「恭喜你中獎了！」，它會直接當機。

### 詞袋模型 (Bag-of-Words, BoW)

為了讓電腦「讀懂」文字，我們採用一種最簡單粗暴的方法：**詞袋模型**。

想像你有一個巨大的袋子，把所有郵件裡的字都剪下來丟進去，搖一搖，忽略它們出現的順序，只在乎**「什麼詞出現了」**以及**「出現了幾次」**。

Let's visualize it:

* **郵件 A**: "中獎, 中獎, 快領取"
* **郵件 B**: "快領取, 優惠"

轉換成矩陣就會變成這樣：

| | 「中獎」次數 | 「快領取」次數 | 「優惠」次數 |
| :--- | :---: | :---: | :---: |
| **郵件 A** | **2** | 1 | 0 |
| **郵件 B** | 0 | 1 | **1** |

現在，每一封郵件都變成了一個由數字組成的向量（一行數據），電腦終於可以處理了！這在 Python 中由 `CountVectorizer` 來實現。

*(註：為了簡化演示，我們在程式碼中使用了字元級別的 n-gram 來切分中文，實際應用中通常會使用 Jieba 等斷詞庫。)*

---

## 1. 理論基礎：簡單到「天真」的假設

貝氏分類器的核心當然是**貝氏定理 (Bayes' Theorem)**。

別怕數學公式，我們用垃圾郵件的例子來看，它其實超直觀：

$$P(\text{垃圾} | \text{文字內容}) = \frac{P(\text{文字內容} | \text{垃圾}) \times P(\text{垃圾})}{P(\text{文字內容})}$$

我們想求的是左邊：**「當我看到這封信的文字內容時，它是垃圾信的機率有多少？」** (這叫後驗機率)

為了算出這個，我們需要知道右邊三件事：
1.  **$P(\text{垃圾})$**：**先驗機率 (Prior)**。不管內容是什麼，隨便抓一封信是垃圾信的機率有多大？（例如全球郵件有 80% 是垃圾信，那這就是 0.8）。
2.  **$P(\text{文字內容} | \text{垃圾})$**：**似然性 (Likelihood)**。如果已知這是一封垃圾信，出現這些文字（例如「中獎」、「比特幣」）的機率有多大？這是模型要從訓練資料中學得的關鍵。
3.  **$P(\text{文字內容})$**：標準化常數。這封信的文字內容在所有郵件中出現的機率。通常在比較分類時可以忽略。

### 為什麼叫「天真 (Naive)」？

計算 $P(\text{文字內容} | \text{垃圾})$ 非常困難。因為「文字內容」是一串詞的組合，例如 $P(\text{「恭喜」, 「你」, 「中獎」} | \text{垃圾})$。要計算這些詞同時出現的機率，計算量會爆炸。

為了簡化計算，貝氏分類器做了一個超級強大但**非常不切實際**的假設：

> **「天真」假設：郵件裡的每一個詞都是相互獨立的，互不影響。**

也就是說，它認為「恭喜」出現的機率，跟後面有沒有接「中獎」完全沒關係。這當然是錯的！人類語言充滿了上下文關聯。

但神奇的是，雖然這個假設很天真，但在實務上（尤其是文本分類），**樸素貝氏分類器 (Naive Bayes Classifier) 的效果竟然出奇地好**，而且計算速度飛快！

### 1.1 Python 程式碼實作
完整程式連結：[Day07_NaiveBayes_Spam.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/Day07_NaiveBayes_Spam.py)
*(程式碼包含文字向量化 Demo 與特徵重要性分析)*

---

## 2. 實驗結果：透視貝氏的腦袋

我們用一個迷你的合成資料集訓練了模型，來看看結果。

![Naive Bayes Results](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/pic/image_f7c8e2.png?raw=true)
*(註：由於資料集很小，每次隨機切分的結果可能略有不同，重點在於理解圖表意義)*

### 左圖：混淆矩陣 (Confusion Matrix)
在我們的小測試集中，模型表現完美 (Accuracy 100%)。正常信都預測為正常，垃圾信都預測為垃圾。這在真實大數據中很難發生，但證明了貝氏在這種特徵明顯的任務上非常有效。

### 右圖：關鍵詞分析 (Top "Spammy" vs. "Hammy" Words)
這張圖是貝氏模型最棒的地方——**高度的可解釋性**。我們可以打開它的腦袋，看看它到底學到了什麼。

我們計算了每個詞組的「垃圾度得分」(Spamminess Score)。得分越高 (紅色往右)，代表這個詞越常在垃圾信中出現。

* **紅色長條 (Spammy Words)**：
    * 模型學到了諸如 **「中獎」、「點擊」、「獨家」、「比特幣」** 等詞組是非常強烈的垃圾信訊號。
    * 這完全符合我們的直覺！當模型看到新郵件裡有這些詞，它計算出的 $P(\text{文字內容} | \text{垃圾})$ 就會飆高。

* **藍色長條 (Hammy Words)**：
    * 相反地，諸如 **「明天」、「開會」、「報告」、「晚餐」** 等詞組，則是正常郵件的特徵。

貝氏分類器就是這樣一個單純的模型：它不理解語義，它只是在**比較機率**。如果一封信裡「紅色詞彙」的總機率乘積大於「藍色詞彙」，它就蓋上「垃圾信」的印章。

---

## 3. 總結 (Conclusion)

今天我們跨越了兩個里程碑：處理了**文字資料**，並引入了**機率模型**。

* **貝氏分類器**的優點：
    * **速度快**：訓練和預測都非常快，適合即時應用。
    * **小數據表現好**：需要的訓練資料相對較少。
    * **可解釋性強**：我們可以知道哪些詞導致了分類結果。
    * **處理高維資料**：文字向量化後特徵常常上萬個，這對其他模型是負擔，但對貝氏來說還好。
* **缺點**：
    * **「天真」假設**：忽略詞語間的關聯，在複雜語義任務上表現不佳（例如情感分析裡的「不喜歡」，貝氏可能會把「不」和「喜歡」分開看）。

### 模型的演進圖譜

回顧我們學過的分類模型，可以發現它們看待世界的方式截然不同：

| 模型 | 視角 | 核心思想 |
| :--- | :--- | :--- |
| **邏輯回歸 (Day 04)** | 幾何視角 | 試圖畫一條**直線**邊界。 |
| **KNN (Day 05)** | 幾何視角 | **記憶**資料，看鄰居是誰。 |
| **SVM (Day 06)** | 幾何視角 | 尋找最完美的**寬邊界** (常涉及高維投影)。 |
| **貝氏分類 (Day 07)** | **機率視角** | 計算特徵出現的**條件機率**。 |

### Next Step:
我們已經看過了幾何和機率的視角。接下來，我們要進入一個更符合人類決策邏輯的領域。

想像你在玩「二十個問題」遊戲，你透過一系列「是/否」的問題來猜測答案。這就是我們下一章的主題——**Day 08 決策樹 (Decision Tree)**。我們將看到模型如何像畫流程圖一樣，一步步切分資料，找出鐵達尼號的生存法則！