# Day 37: 深度強化學習 - Deep Q-Network (DQN)

## 1. 前言：當 Q 表寫不下的時候
昨天我們學了 **Q-Learning**，用一張表 (Q-Table) 來記錄每個狀態的分數。
這在簡單的迷宮 (6 個狀態) 很有效。但如果是 **圍棋** 呢？
*   圍棋的狀態數：$10^{170}$ (比宇宙原子總數還多)。
*   **問題**：你的記憶體根本存不下這張表！

為了解決這個問題，DeepMind 在 2013 年提出了 **DQN (Deep Q-Network)**。
*   **核心想法**：既然存不下表，那我們用一個 **神經網路 (Neural Network)** 來「背」這張表！
*   **輸入**：狀態 (畫面或數值)。
*   **輸出**：每個動作的 Q 值。

## 2. DQN 的三大法寶
要讓神經網路學會玩遊戲並不容易，DQN 用了三個關鍵技術來穩定訓練：

### 2.1 Q-Network (用 AI 取代查表)
*   **以前**：查表 `Q_Table[state][action]`。
*   **現在**：問 AI `model(state)` -> 輸出所有動作的分數 `[Q_left, Q_right]`。

### 2.2 Experience Replay (經驗回放)
*   **問題**：玩遊戲的資料是連續的 (這一秒的畫面跟下一秒很像)。神經網路討厭這種高度相關的資料 (會學壞)。
*   **解法**：把玩過的經驗 $(S, A, R, S')$ 全部丟進一個 **「回憶庫」 (Replay Buffer)**。
*   **訓練時**：從庫裡 **隨機抽樣 (Random Batch)** 出來學習。這樣可以打亂時間順序，讓 AI 學得更全面。

### 2.3 Target Network (固定目標)
*   **問題**：我們用 Q 網路來預測 Q 值，又用同一個 Q 網路來計算目標 (Target)。這就像「左腳踩右腳」想飛起來，很不穩定。
*   **解法**：準備兩個網路。
    *   **Policy Net (學生)**：負責玩遊戲，隨時更新參數。
    *   **Target Net (老師)**：負責算分數，參數固定一段時間 (例如每 100 步) 才從學生那裡複製過來。

## 3. 實戰：倒立擺 (CartPole)
我們要挑戰 OpenAI Gym 的經典遊戲：**CartPole-v1**。
*   **目標**：控制小車左右移動，讓桿子保持直立不倒。
*   **狀態 (4 維)**：小車位置、小車速度、桿子角度、桿子角速度。
*   **動作 (2 種)**：往左推、往右推。
*   **獎勵**：桿子每堅持一秒不倒，得 +1 分。

### 3.1 程式碼架構 (`DQN_CartPole.py`)
1.  **QNetwork**：一個簡單的 MLP (4 -> 128 -> 2)。
2.  **ReplayBuffer**：用 `deque` 儲存經驗。
3.  **Agent**：
    *   `act()`: 決定動作 (Epsilon-Greedy)。
    *   `learn()`: 從 Buffer 抽樣並訓練網路。
4.  **Main Loop**：讓 Agent 玩遊戲，並記錄每一回合撐了多久。

## 4. 執行結果預期
*   **一開始**：桿子馬上就倒了 (Score ~10)。
*   **訓練幾百回合後**：Agent 學會了微調平衡。
*   **最終**：桿子可以屹立不搖 (Score > 200)。

## 5. 下一關預告
DQN 是 Value-Based 的方法 (算分數)。
Day 38 我們將介紹 **Policy Gradient (策略梯度)**。
它不看分數，而是直接學習「動作的機率」。
這是現代 LLM (如 ChatGPT) 的 RLHF 基礎喔！
