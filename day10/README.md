# Day 10：主成分分析 (PCA) - 資料的瘦身魔法

## 0. 歷史小百科：
在機器學習的世界裡，我們常常面臨「維度災難」(Curse of Dimensionality) —— 特徵太多，但資料樣本不足。這時，我們需要一種方法來為資料「瘦身」。

**主成分分析 (Principal Component Analysis, PCA)** 是最古老也最經典的線性降維技術。它的起源可以追溯到 1901 年由現代統計學之父 Karl Pearson 提出，後來在 1930 年代由 Harold Hotelling 進一步發展。

PCA 不是用來「預測」的，而是一種**無監督 (Unsupervised)** 的方法，用來探索資料的內部結構，找出看待資料「最好的角度」。

## 1. 資料集來源
**資料集來源**：[UCI Machine Learning Repository - German Credit Data](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))
這份資料集是金融風控領域的「Hello World」，由漢堡大學 (University of Hamburg) 的 Hans Hofmann 教授於 1994 年捐贈。它包含 1,000 筆真實的貸款紀錄，包含以下關鍵欄位，每一個都是徵信審核的重點：
![German Credit Data](https://github.com/ksharry/30-Days-Of-ML/blob/main/day7/pic/7-1.jpg?raw=true)

## 2. 原理
PCA 的核心思想非常直觀：**找出資料中「資訊量」最大的方向，並拋棄資訊量小的方向。**

在統計學中，「變異數 (Variance)」代表了資訊量。如果一個特徵大家數值都差不多（變異數小），那這個特徵就沒什麼區別能力；反之，如果數據分佈很廣（變異數大），則包含較多資訊。

### 核心概念
1.  **資料標準化 (Standardization)**：**這是最重要的一步！** 因為 PCA 是根據變異數大小來決定重要性的。如果「貸款金額」是幾千幾萬，「年齡」只有幾十，不做標準化，「貸款金額」會主導整個 PCA 結果。我們必須把所有特徵縮放到同一個起跑線（平均值 0，標準差 1）。
2.  **尋找主成分 (Principal Components)**：
    * **PC1 (第一主成分)**：在資料空間中找到一個軸，使得數據投影到這個軸上後，變異數最大（拉得最開）。
    * **PC2 (第二主成分)**：找到第二個軸，它必須與 PC1 **垂直 (正交)**，且在剩下方向中變異數最大。
    * 以此類推，產生與原始特徵數量相同的新軸。
3.  **降維**：保留前幾個變異數最大的主成分（例如前 90%），丟棄剩下的。

> **信用風控領域的警語**：
> PCA 雖然能降維並解決共線性問題，但它產生出來的「主成分」是原始特徵的線性組合，**失去了物理意義**。你無法向客戶解釋「因為你的 PC3 分數太高而被拒絕」。因此，在需要高度可解釋性的最終風控模型中，需謹慎使用。

## 3. 實戰：信用違約預測 (輔助建模)
今天我們不直接用 PCA 做分類，而是用它來處理資料，再餵給邏輯回歸模型。

### Python 程式碼實作
### 建模成績單 
我們使用降維後的 13 個特徵進行訓練：
* 🔹 **訓練集準確率 (Training Acc): 0.7657**
* 🔹 **測試集準確率 (Test Acc): 0.7500**

我們將原本 20 個維度的資料，壓縮到只剩下能解釋 90% 變異量的維度。
完整程式連結：[PCA_analysis.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day10/PCA_analysis.py)

## 4. 模型評估
### 降維效果分析
![threshold](https://github.com/ksharry/30-Days-Of-ML/blob/main/day10/pic/10-1.png?raw=true)
執行程式後，我們發現：
* **原始特徵維度**: 20
* **保留 90% 變異量後的維度**: 13
* PCA 成功將特徵數量減少了約 35%，同時保留了絕大部分的資訊。

### 圖解分析：我們可以把 20 維畫出來了！
![分布](https://github.com/ksharry/30-Days-Of-ML/blob/main/day10/pic/10-2.png?raw=true)
我們無法想像 20 維空間，但 PCA 可以幫我們把資料壓扁到 2D 平面上看看樣子：

* **X 軸 (PC1)**：解釋了最多的變異 (約 15%)。
* **Y 軸 (PC2)**：解釋了第二多的變異 (約 9%)。
* **觀察**：紅色點 (違約) 和綠色點 (正常) 在這兩個主成分的平面上**並沒有明顯分開**，而是混雜在一起。這直觀地告訴我們，這個信用資料集是比較困難的，單靠最重要的兩個線性組合無法輕易區分好壞客戶。

雖然準確率沒有昨天強大的 XGBoost (77%+) 高，但這個結果與使用全部 20 個特徵的邏輯回歸相近。這證明了 PCA 成功地在減少特徵數量的同時，保留了模型的預測能力。

## 5. 戰略總結:模型訓練的火箭發射之旅
最後，讓我們引用 AI 大師 **吳恩達 (Andrew Ng)** 的經典圖表，來重新審視我們學到的模型
![rocket](https://github.com/ksharry/30-Days-Of-ML/blob/main/day2/pic/2-6.jpg?raw=true)

PCA 在這個火箭系統中，扮演的是「燃料過濾器」或「結構優化師」的角色。

### 5.1 流程一：推力不足，無法升空 (Underfitting 迴圈)
* **PCA 的視角**：如果我們降維降得太厲害，例如從 20 維硬降到只剩 2 維，雖然變簡單了，但丟失了太多關鍵資訊（也許丟掉了區分違約的重要細節）。這會導致模型學不到東西。
* **行動 (Action)**：增加保留的主成分數量 (提高解釋變異量的門檻，例如從 80% 提高到 95%)。

### 5.2 流程二：動力太強，失控亂飛 (Overfitting 迴圈)
* **PCA 的視角**：當原始特徵非常多且包含許多雜訊或高度相關的特徵時，模型容易過度擬合。PCA 可以合併相關特徵，去除雜訊方向，從而**減輕過度擬合**。
* **行動 (Action)**：適度使用 PCA 降維，作為一種正則化的手段。

### 5.3 流程三：完美入軌 (The Sweet Spot)
* **設定**：找到一個平衡點（例如本例中的 90% 變異量，保留 13 個組件），既簡化了模型，又保留了足夠的訊號。
* **結果**：**完成！** 獲得一個更穩健、計算更快的模型（儘管損失了可解釋性）。

## 6. 總結與比較
我們回顧一下目前的戰況。請注意，PCA 本身不是分類器，這裡展示的是「PCA + 邏輯回歸」的結果。
| 模型 | Day | 特性 | 德國信用資料準確率* |
| :--- | :---: | :--- | :---: |
| **決策樹** | 07 | 白箱模型，可畫圖解釋 | 70.00% |
| **隨機森林** | 08 | 多樹投票，穩定運行 | 76.00% |
| **XGBoost** | 09 | 精度提升 | 77.33% |
| **LightGBM** | 09 | 海量數據首選 | 78.00% |
| **PCA + LR** | **10** | **降維打擊，視覺化工具** | **75.00%** |

**Next Day 預告：**
監督學習我們已經看了很多，明天我們換個口味，看看無監督學習中的另一位巨星：**K-Means 分群**，看看能不能自動把客戶分成「VIP客戶」、「普通客戶」和「高風險客戶」？