# Day 06：支援向量機 (SVM) —— 尋找最寬的那條路

## 0. 歷史小故事：從冷戰到黃金時代

支援向量機 (SVM) 的故事起源於冷戰時期的蘇聯。1960 年代，一位名叫 **弗拉基米爾·瓦普尼克 (Vladimir Vapnik)** 的數學家開始思考一個根本性的問題：如何讓機器不僅是「死記硬背」訓練數據，而是真正學會「舉一反三」的泛化能力？

當時的主流觀點是「經驗風險最小化 (Empirical Risk Minimization)」，也就是盡可能減少在訓練集上的錯誤。但 Vapnik 認為這還不夠，他提出了 **「結構風險最小化 (Structural Risk Minimization)」** 的革命性概念。他認為，一個好的模型不僅要分對訓練數據，還要在分類邊界兩側留出最大的「安全間距 (Margin)」，這樣在面對未知的測試數據時，才不容易出錯。

這個理論在當時並未引起太多關注。直到 1990 年代，隨著計算能力的提升和「核函數技巧 (Kernel Trick)」的引入，Vapnik 的理論終於開花結果，誕生了我們今天所知的 SVM。在深度學習尚未普及的年代，SVM 憑借其堅實的數學基礎和卓越的性能，統治了機器學習領域長達十多年，被譽為統計學習理論的巔峰之作。

---

## 1. 前言：不僅要分對，還要分得漂亮

回顧一下我們在鐵達尼號上的進展：
* **Day 04 邏輯回歸**：畫了一條直線，勉強把人分開，準確率約 81%。
* **Day 05 KNN**：放棄畫線，用「看鄰居」的方式捕捉到了非線性的形狀，準確率提升到約 82.6%。

今天，我們要介紹這一位重量級選手——**支援向量機 (Support Vector Machine, SVM)**。

SVM 的核心思想非常優雅：
> 如果邏輯回歸只是想找到「一條」能區分生死的界線，那麼 SVM 的目標就是找到 **「最完美的一條」** 界線。

什麼叫完美？就是這條界線不僅能分開資料，而且離兩邊的資料點都 **越遠越好**，就像在生死之間開闢了一條 **「最寬的馬路 (Max Margin)」**。

### Python 程式碼實作
完整程式連結：[SVM_Titanic.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/SVM_Titanic.py)
*(程式碼包含兩個重要的實驗：線性邊界的比較，以及核函數的應用)*

---

## 2. 理論基礎：解構最寬馬路 (The Mathematics of Margins)

SVM 的目標不僅僅是畫一條線，而是要畫出 **「路寬 (Margin) 最大」** 的那條線。讓我們看看數學上是如何定義這條路的。

### 2.1 核心公式：決策邊界

我們定義這條「馬路的中線」（決策邊界）為：
$$w \cdot x + b = 0$$

而「馬路的兩側邊緣」（也就是穿過支援向量的那兩條虛線）被定義為：
* **正向邊緣**：$w \cdot x + b = 1$
* **負向邊緣**：$w \cdot x + b = -1$

**變數對照表：**
* **$x$ (Features)**：乘客的特徵向量 (年齡, 票價...)。
* **$w$ (Weights)**：**法向量 (Normal Vector)**。它決定了馬路的方向（是橫著走還是斜著走）。
* **$b$ (Bias)**：偏差值。它決定了馬路的位置（離原點多遠）。

### 2.2 為什麼要「最小化 $w$」？

根據幾何學公式，這兩條邊緣線之間的垂直距離（路寬, Margin）等於：

$$\text{Margin} = \frac{2}{\|w\|}$$

這公式告訴我們一個反直覺的真理：
> 如果想要 **最大化** 路寬 (Margin)，我們就必須 **最小化** 權重向量的長度 $\|w\|$。

這就是為什麼 SVM 的損失函數長這樣：
$$J(w) = \frac{1}{2}\|w\|^2 + C \times \text{(分類錯誤的懲罰)}$$

我們希望 $\|w\|$ 越小越好（路越寬越好），但同時我們也希望分類錯誤越少越好。這中間的權衡，就由參數 $C$ 來控制。

### 2.3 關鍵參數解析 (C & Gamma)

在使用 `sklearn.svm.SVC` 時，有兩個最重要的「旋鈕」決定了模型的行為：

| 參數 | 名稱 | 意義與影響 | 物理比喻 |
| :--- | :--- | :--- | :--- |
| **C** | **懲罰係數**<br>(Regularization) | **控制對「錯誤」的容忍度**。<br>● **C 很大**：嚴格的老師。不允許任何學生出錯 (Hard Margin)。邊界會變得很窄、很扭曲，容易過擬合。<br>● **C 很小**：佛系的老師。允許一些錯誤，追求更寬、更平滑的邊界 (Soft Margin)。泛化能力通常較好。 | **路障的材質**<br>大 C = 鋼鐵路障 (撞不得)<br>小 C = 交通錐 (壓過去也還好) |
| **Gamma** | **核函數係數**<br>(RBF Kernel only) | **控制單一訓練樣本的「影響力範圍」**。<br>● **Gamma 大**：影響範圍小。只有附近的點能影響邊界。邊界會變得破碎、像狗啃的一樣 (Overfitting)。<br>● **Gamma 小**：影響範圍大。遠處的點也能互相呼應。邊界會很平滑、圓潤。 | **近視的程度**<br>大 Gamma = 高度近視 (只看清眼前的點)<br>小 Gamma = 視力 2.0 (看得到大局) |

---

## 3. 實驗一：直線的對決 (Logistic vs Linear SVM)

為了視覺化，我們再次將資料簡化到 2D 平面 (Age vs Fare)，看看邏輯回歸和線性 SVM 畫出來的線有什麼不同。

![Logistic vs Linear SVM](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/pic/6-1.jpg?raw=true)

### 左圖：邏輯回歸 (Logistic Regression)
* **解讀**：邏輯回歸找到了一條能區分藍點 (生存) 和紅點 (死亡) 的黑線。但注意看，這條線**非常貼近**下方的紅點群。
* **隱憂**：這條線雖然現在分對了，但它「缺乏安全感」。如果測試資料稍微有一點偏差，很容易就會跨過這條線而被誤判。它只求及格，不求高分。

### 右圖：線性 SVM (Linear SVM)
* **解讀**：SVM 也畫了一條直線。但明顯地，這條線位於紅點群和藍點群的**正中央**，距離兩邊都保持了最大的安全距離。
* **支援向量 (關鍵點)**：請注意圖中被黑色圓圈圈起來的點。
    * 下方的幾個紅點（低票價、各年齡層的死亡者）。
    * 上方的幾個藍點（中高票價的倖存者）。
    * SVM 的這條線，完全是由這幾個「柱子」撐起來並決定的。其他的點對邊界沒有影響。
* **意義**：這就是 **「最大邊界 (Max Margin)」**。這條線是最穩健 (Robust) 的，對未知資料的容錯率最高。

---

## 4. 實驗二：核函數的魔法 (The Kernel Trick)

SVM 最強大的地方還不在於畫直線，而在於它處理非線性問題的能力。

如果資料像 Day 05 看到的那樣，是直線切不開的怎麼辦？SVM 有一個絕招，叫做 **「核函數技巧 (Kernel Trick)」**。

> **直觀理解**：想像桌上散落著紅豆和綠豆，混雜在一起。你在桌面上（二維空間）怎麼畫線都分不開。這時，SVM 用力拍了一下桌子，豆子們被震飛到了空中（三維空間）。在空中的某一瞬間，紅豆和綠豆可能就有了高度差，這時你用一個平面（像切水果一樣）就能把它們完美分開了。

核函數就是那個「拍桌子」的魔法，它能在不增加計算負擔的情況下，幫我們把資料投影到高維空間。

![Linear SVM vs RBF SVM](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/pic/6-2.jpg?raw=true)

### 左圖：線性 SVM (Linear Kernel)
* 這就是剛剛那張圖。在二維平面上硬要畫直線，只能做到這樣，中間混雜的區域無法處理。

### 右圖：徑向基核函數 SVM (RBF Kernel)
* **魔法展現**：我們使用了最常用的 **RBF (Radial Basis Function)** 核函數。
* **解讀**：
    * 邊界不再是直線，而是變成了封閉的曲線或複雜的形狀。
    * 它成功地將中間下方那一群密集的紅點（低票價死亡群體）包圍了起來。
    * 這效果是不是有點像 Day 05 的 KNN？沒錯，RBF 核函數的 SVM 在行為上與 KNN 有異曲同工之妙，都能捕捉局部的非線性特徵，但 SVM 的數學基礎通常帶來更滑順、更穩健的邊界。

---
## 5. 額外實驗：三維空間的魔法 (3D Visualization)

為了親眼見證「核函數」是如何將資料投影到高維空間並進行切割的，我們在程式碼中加碼了一個實驗。我們引入了第三個關鍵特徵 **Pclass (艙等)**，將戰場從 2D 平面升級到了 3D 立體空間。

![SVM 3D Decision Space](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/pic/6-3.jpg?raw=true)

### 圖表深度解析：
這張圖展示了 SVM (RBF Kernel) 在三維空間中的決策邏輯：

1.  **三個維度 (Axes)**：
    * **X軸 (Age)**：年齡。
    * **Y軸 (Fare)**：票價。
    * **Z軸 (Pclass)**：艙等 (經過標準化處理)。

2.  **資料點 (Scatter Points)**：
    * **🔴 紅點 (Dead)**：實際死亡的乘客。
    * **🔵 藍點 (Survived)**：實際倖存的乘客。

3.  **藍色霧氣 (The Blue Fog) —— 關鍵所在！**
    * 這團半透明的藍色霧氣，代表模型預測的 **「生存區域 (Decision Volume)」**。
    * 只要乘客的數據點落在這團霧氣裡面，SVM 就會判定他能活下來；落在外面，則判定死亡。

### 觀察重點：
請注意這團霧氣的形狀。**它並不是一個平整的切面（像用紙切蛋糕那樣）**。
它看起來像是有層次、不規則的「雲團」，精準地包裹住了大部分的藍點（倖存者），同時避開了紅點（死亡者）。

這證實了 **RBF 核函數** 的威力：它不是在畫死板的直線或平面，而是根據資料的分佈，在多維空間中塑造成一個 **「複雜的立體結界」**。這就是為什麼 SVM 能比邏輯回歸處理更複雜的非線性問題。

---

## 6. 總結與比較 (Conclusion)

我們使用所有特徵在測試集上進行了最終評估：
* **Final SVM (RBF Kernel) Accuracy: 82.12%**

讓我們看看目前的排行榜：

| 模型 | Day | 特性 | 鐵達尼號準確率 (約) |
| :--- | :---: | :--- | :---: |
| **邏輯回歸** | 04 | 畫直線，只求分開 | 81.0% |
| **KNN (K=17)** | 05 | 看鄰居，非線性直覺 | 81.6% |
| **SVM (RBF)** | **06** | **高維魔法，尋找最寬邊界** | **82.1%** |

SVM 結合了「最大邊界」的穩健性與「核函數」的非線性能力，在這個資料集上取得了目前最好的成績。

### SVM 的優缺點
* **優點**：數學理論嚴謹、對小樣本數據效果好、能處理高維和非線性問題、不易過擬合（因為有最大邊界保護）。
* **缺點**：對大規模數據集計算慢、對缺失值和無標準化的數據敏感、**模型難以解釋**（黑盒子）。

---

## 7. 戰略總結：模型訓練的火箭發射之旅

在學習了多種模型後，我們可以用一個更宏觀的視角來看待機器學習模型的訓練過程。這張圖將模型的訓練和調參比喻為一次火箭發射任務，形象地說明了我們在追求高性能模型時會遇到的挑戰和應對策略。

![模型訓練與參數調整的火箭發射之旅](https://github.com/ksharry/30-Days-Of-ML/blob/main/day2/pic/2-6.jpg?raw=true)

這個流程圖展示了從開始訓練到最終模型完成的標準作業程序 (SOP)：

1.  **開始訓練**：
    一切的起點。我們選擇一個模型（例如 SVM）和一組初始參數（例如線性核函數），並準備好訓練數據（燃料）。

2.  **第一關：訓練集表現好嗎？**
    * 這是檢查我們的「火箭引擎」推力是否足夠。
    * **否 (欠擬合 Underfitting)**：如果模型連訓練數據都學不好，說明它的學習能力太弱，就像火箭推力不足，無法升空。
        * **對策**：**增加複雜度**。對於 SVM 來說，這意味著放棄簡單的線性核函數，改用更強大的 **RBF 核函數**，讓模型能夠在更高維的空間中找到複雜的決策邊界。這就像是換了一個更大的引擎。

3.  **第二關：測試/驗證集表現好嗎？**
    * 如果火箭成功升空（訓練集表現好），接下來要看它飛得穩不穩，能否適應未知的環境（測試數據）。
    * **否 (過擬合 Overfitting)**：如果模型在訓練集上表現完美，但在測試集上卻很差，說明它「死記硬背」了訓練數據中的雜訊，泛化能力差。這就像火箭動力太強卻失去了控制，在空中亂飛。
        * **對策**：**限制複雜度或增加正則化**。對於 SVM，我們可以調整 **`C` 參數**（懲罰系數）。減小 `C` 值會容許更多錯誤，讓邊界更平滑、更寬，從而提高泛化能力。這就像是改進了火箭的穩定系統。另一個方法是給火箭更多的「燃料」，也就是**增加訓練資料**，讓模型學到更普遍的規律。

4.  **表現良好 -> 成功入軌 -> 完成！**
    * 當模型在訓練集和測試集上都表現良好時，我們就找到了最佳的平衡點（甜蜜點）。這時火箭成功入軌，穩定運行，我們的模型訓練任務也就完成了。

這張圖完美地總結了我們在 Day 03 (正則化)、Day 05 (KNN 調參) 和今天 Day 06 (SVM 核函數與調參) 中所做的一切努力：不斷地在「欠擬合」和「過擬合」之間尋找那個讓模型性能達到最優的平衡點。

### Next Step:
到目前為止，我們的模型（LogReg 的係數、KNN 的鄰居、SVM 的高維空間）對人類來說都越來越難以理解了。

有沒有一種模型，它的決策過程像人類一樣清晰，充滿了「如果...就...」的規則？
**Day 07 - 決策樹 (Decision Tree)** 將登場。我們將畫出像流程圖一樣的樹狀結構，一眼看穿鐵達尼號的生存法則！