# Day 06：支援向量機 (SVM) —— 尋找最寬的那條路

## 0. 歷史小故事：從冷戰到黃金時代

支援向量機 (SVM) 的故事起源於冷戰時期的蘇聯。1960 年代，一位名叫 **弗拉基米爾·瓦普尼克 (Vladimir Vapnik)** 的數學家開始思考一個根本性的問題：如何讓機器不僅是「死記硬背」訓練數據，而是真正學會「舉一反三」的泛化能力？

當時的主流觀點是「經驗風險最小化 (Empirical Risk Minimization)」，也就是盡可能減少在訓練集上的錯誤。但 Vapnik 認為這還不夠，他提出了 **「結構風險最小化 (Structural Risk Minimization)」** 的革命性概念。他認為，一個好的模型不僅要分對訓練數據，還要在分類邊界兩側留出最大的「安全間距 (Margin)」，這樣在面對未知的測試數據時，才不容易出錯。

這個理論在當時並未引起太多關注。直到 1990 年代，隨著計算能力的提升和「核函數技巧 (Kernel Trick)」的引入，Vapnik 的理論終於開花結果，誕生了我們今天所知的 SVM。在深度學習尚未普及的年代，SVM 憑借其堅實的數學基礎和卓越的性能，統治了機器學習領域長達十多年，被譽為統計學習理論的巔峰之作。

---

## 1. 前言：不僅要分對，還要分得漂亮

回顧一下我們在鐵達尼號上的進展：
* **Day 04 邏輯回歸**：畫了一條直線，勉強把人分開，準確率約 81%。
* **Day 05 KNN**：放棄畫線，用「看鄰居」的方式捕捉到了非線性的形狀，準確率提升到約 82.6%。

今天，我們要介紹這一位重量級選手——**支援向量機 (Support Vector Machine, SVM)**。

SVM 的核心思想非常優雅：
> 如果邏輯回歸只是想找到「一條」能區分生死的界線，那麼 SVM 的目標就是找到 **「最完美的一條」** 界線。

什麼叫完美？就是這條界線不僅能分開資料，而且離兩邊的資料點都 **越遠越好**，就像在生死之間開闢了一條 **「最寬的馬路 (Max Margin)」**。

---

## 2. 理論基礎：支撐起馬路的柱子

SVM 認為，決定這條生死邊界的，並不是所有的乘客資料，而是那些**處於最前線、最容易被分錯的關鍵人物**。

想像一下鐵達尼號的甲板上，一邊站著必死無疑的人（如三等艙男丁），另一邊站著必能獲救的人（如頭等艙婦女）。而在中間地帶，混雜著一些「邊緣案例」：可能是稍微年長的一等艙男性，或是帶著孩子的三等艙女性。

SVM 的眼裡只有這些邊緣人。這些關鍵的資料點，就叫做 **「支援向量 (Support Vectors)」**。它們就像是馬路兩邊的柱子，撐起了整條邊界的寬度。

### 2.1 Python 程式碼實作
完整程式連結：[SVM_Titanic.py](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/SVM_Titanic.py)
*(程式碼包含兩個重要的實驗：線性邊界的比較，以及核函數的應用)*

---

## 3. 實驗一：直線的對決 (Logistic vs Linear SVM)

為了視覺化，我們再次將資料簡化到 2D 平面 (Age vs Fare)，看看邏輯回歸和線性 SVM 畫出來的線有什麼不同。

![Logistic vs Linear SVM](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/pic/6-1.jpg?raw=true)

### 左圖：邏輯回歸 (Logistic Regression)
* **解讀**：邏輯回歸找到了一條能區分藍點 (生存) 和紅點 (死亡) 的黑線。但注意看，這條線**非常貼近**下方的紅點群。
* **隱憂**：這條線雖然現在分對了，但它「缺乏安全感」。如果測試資料稍微有一點偏差，很容易就會跨過這條線而被誤判。它只求及格，不求高分。

### 右圖：線性 SVM (Linear SVM)
* **解讀**：SVM 也畫了一條直線。但明顯地，這條線位於紅點群和藍點群的**正中央**，距離兩邊都保持了最大的安全距離。
* **支援向量 (關鍵點)**：請注意圖中被黑色圓圈圈起來的點。
    * 下方的幾個紅點（低票價、各年齡層的死亡者）。
    * 上方的幾個藍點（中高票價的倖存者）。
    * SVM 的這條線，完全是由這幾個「柱子」撐起來並決定的。其他的點對邊界沒有影響。
* **意義**：這就是 **「最大邊界 (Max Margin)」**。這條線是最穩健 (Robust) 的，對未知資料的容錯率最高。

---

## 4. 實驗二：核函數的魔法 (The Kernel Trick)

SVM 最強大的地方還不在於畫直線，而在於它處理非線性問題的能力。

如果資料像 Day 05 看到的那樣，是直線切不開的怎麼辦？SVM 有一個絕招，叫做 **「核函數技巧 (Kernel Trick)」**。

> **直觀理解**：想像桌上散落著紅豆和綠豆，混雜在一起。你在桌面上（二維空間）怎麼畫線都分不開。這時，SVM 用力拍了一下桌子，豆子們被震飛到了空中（三維空間）。在空中的某一瞬間，紅豆和綠豆可能就有了高度差，這時你用一個平面（像切水果一樣）就能把它們完美分開了。

核函數就是那個「拍桌子」的魔法，它能在不增加計算負擔的情況下，幫我們把資料投影到高維空間。

![Linear SVM vs RBF SVM](https://github.com/ksharry/30-Days-Of-ML/blob/main/day6/pic/6-2.jpg?raw=true)

### 左圖：線性 SVM (Linear Kernel)
* 這就是剛剛那張圖。在二維平面上硬要畫直線，只能做到這樣，中間混雜的區域無法處理。

### 右圖：徑向基核函數 SVM (RBF Kernel)
* **魔法展現**：我們使用了最常用的 **RBF (Radial Basis Function)** 核函數。
* **解讀**：
    * 邊界不再是直線，而是變成了封閉的曲線或複雜的形狀。
    * 它成功地將中間下方那一群密集的紅點（低票價死亡群體）包圍了起來。
    * 這效果是不是有點像 Day 05 的 KNN？沒錯，RBF 核函數的 SVM 在行為上與 KNN 有異曲同工之妙，都能捕捉局部的非線性特徵，但 SVM 的數學基礎通常帶來更滑順、更穩健的邊界。

---

## 5. 總結與比較 (Conclusion)

我們使用所有特徵在測試集上進行了最終評估：
* **Final SVM (RBF Kernel) Accuracy: 82.12%**

讓我們看看目前的排行榜：

| 模型 | Day | 特性 | 鐵達尼號準確率 (約) |
| :--- | :---: | :--- | :---: |
| **邏輯回歸** | 04 | 畫直線，只求分開 | 81.0% |
| **KNN (K=17)** | 05 | 看鄰居，非線性直覺 | 81.6% |
| **SVM (RBF)** | **06** | **高維魔法，尋找最寬邊界** | **82.1%** |

SVM 結合了「最大邊界」的穩健性與「核函數」的非線性能力，在這個資料集上取得了目前最好的成績。

### SVM 的優缺點
* **優點**：數學理論嚴謹、對小樣本數據效果好、能處理高維和非線性問題、不易過擬合（因為有最大邊界保護）。
* **缺點**：對大規模數據集計算慢、對缺失值和無標準化的數據敏感、**模型難以解釋**（黑盒子）。

---

## 6. 戰略總結：模型訓練的火箭發射之旅

在學習了多種模型後，我們可以用一個更宏觀的視角來看待機器學習模型的訓練過程。這張圖將模型的訓練和調參比喻為一次火箭發射任務，形象地說明了我們在追求高性能模型時會遇到的挑戰和應對策略。

![模型訓練與參數調整的火箭發射之旅](https://github.com/ksharry/30-Days-Of-ML/blob/main/day2/pic/2-6.jpg?raw=true)

這個流程圖展示了從開始訓練到最終模型完成的標準作業程序 (SOP)：

1.  **開始訓練**：
    一切的起點。我們選擇一個模型（例如 SVM）和一組初始參數（例如線性核函數），並準備好訓練數據（燃料）。

2.  **第一關：訓練集表現好嗎？**
    * 這是檢查我們的「火箭引擎」推力是否足夠。
    * **否 (欠擬合 Underfitting)**：如果模型連訓練數據都學不好，說明它的學習能力太弱，就像火箭推力不足，無法升空。
        * **對策**：**增加複雜度**。對於 SVM 來說，這意味著放棄簡單的線性核函數，改用更強大的 **RBF 核函數**，讓模型能夠在更高維的空間中找到複雜的決策邊界。這就像是換了一個更大的引擎。

3.  **第二關：測試/驗證集表現好嗎？**
    * 如果火箭成功升空（訓練集表現好），接下來要看它飛得穩不穩，能否適應未知的環境（測試數據）。
    * **否 (過擬合 Overfitting)**：如果模型在訓練集上表現完美，但在測試集上卻很差，說明它「死記硬背」了訓練數據中的雜訊，泛化能力差。這就像火箭動力太強卻失去了控制，在空中亂飛。
        * **對策**：**限制複雜度或增加正則化**。對於 SVM，我們可以調整 **`C` 參數**（懲罰系數）。減小 `C` 值會容許更多錯誤，讓邊界更平滑、更寬，從而提高泛化能力。這就像是改進了火箭的穩定系統。另一個方法是給火箭更多的「燃料」，也就是**增加訓練資料**，讓模型學到更普遍的規律。

4.  **表現良好 -> 成功入軌 -> 完成！**
    * 當模型在訓練集和測試集上都表現良好時，我們就找到了最佳的平衡點（甜蜜點）。這時火箭成功入軌，穩定運行，我們的模型訓練任務也就完成了。

這張圖完美地總結了我們在 Day 03 (正則化)、Day 05 (KNN 調參) 和今天 Day 06 (SVM 核函數與調參) 中所做的一切努力：不斷地在「欠擬合」和「過擬合」之間尋找那個讓模型性能達到最優的平衡點。

### Next Step:
到目前為止，我們的模型（LogReg 的係數、KNN 的鄰居、SVM 的高維空間）對人類來說都越來越難以理解了。

有沒有一種模型，它的決策過程像人類一樣清晰，充滿了「如果...就...」的規則？
**Day 07 - 決策樹 (Decision Tree)** 將登場。我們將畫出像流程圖一樣的樹狀結構，一眼看穿鐵達尼號的生存法則！