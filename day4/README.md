# Day 04：邏輯回歸 (Logistic Regression) —— 生死一線間

## 0. 歷史小故事：人口增長的極限
我們先回到 19 世紀。

當時，馬爾薩斯的人口論認為「人口會呈現幾何級數爆炸增長」。但比利時數學家 **皮埃爾·弗朗索瓦·費爾許爾斯特 (Pierre François Verhulst)** 覺得這不合理，因為資源是有限的，人口不可能無限膨脹，增長速度到最後一定會慢下來並趨於平緩。

於是他提出了一個呈現 **S 型 (Sigmoid)** 的曲線公式 —— **Logistic Function**。

一百多年後，英國統計學家 **大衛·考克斯 (David Cox)** 將這個概念發揚光大，正式提出了 **邏輯回歸 (Logistic Regression)** 模型。他發現這個「S 型曲線」非常完美地解決了機率問題：它能將任何數值強行壓縮在 0% 到 100% 之間，成為了現代分類演算法的基石。

在 Day 02 與 Day 03，我們處理的是房價預測（回歸問題）。但現實生活中，更多時候我們要面對的是 **「是非題」**：
* 這封信是垃圾郵件嗎？(Yes/No)
* 這位病人有癌症嗎？(Positive/Negative)
* **這位乘客能在鐵達尼號倖存嗎？(Survived/Died)**

今天，我們開啟新的故事線 —— **鐵達尼號生存預測**。我們將使用最經典的分類演算法：**邏輯回歸 (Logistic Regression)**。

---

## 1 資料介紹
### 1.1 資料集介紹 (Data Dictionary)

本實作使用 kaggle 競賽預測生存率的 [Titanic](https://www.kaggle.com/competitions/titanic/data) 資料集。

![Titanic](https://github.com/ksharry/30-Days-Of-ML/blob/main/day4/pic/4-1.jpg?raw=true)

| 欄位名稱 (Column) | 說明 (Description) | 備註 (Key) |
| :--- | :--- | :--- |
| **Survived** | 生存與否 | 0 = 死亡, 1 = 生存 |
| **Pclass** | 艙等 (社經地位) | 1 = 頭等艙, 2 = 二等艙, 3 = 三等艙 |
| **Sex** | 性別 | male / female |
| **Age** | 年齡 | 部分資料有缺失 |
| **SibSp** | 同船兄弟姊妹/配偶數 | # of siblings / spouses aboard |
| **Parch** | 同船父母/子女數 | # of parents / children aboard |
| **Ticket** | 船票號碼 | |
| **Fare** | 票價 | |
| **Cabin** | 客艙號碼 | 缺失值較多 |
| **Embarked** | 登船港口 | C = Cherbourg, Q = Queenstown, S = Southampton |

### 1.2 數據預處理 (Data Preprocessing)

我們使用 Python 的 `sklearn` 來實作。為了讓電腦讀懂資料，我們做了兩件重要的預處理：

1.  **填補缺失值 (Missing Values)：** 很多乘客的年齡是空的，我們用「中位數」填補，避免誤導模型。
2.  **獨熱編碼 (One-Hot Encoding)：** 電腦看不懂 "Male" 和 "Female"。我們將其轉換為數字。
邏輯回歸的運作可以拆解成兩個步驟。你剛才看到的那些數學符號，其實每一個都對應到鐵達尼號的具體情境。

## 2 解構神秘公式
### 2.1 第一步：線性計分 (The Linear Score)
模型會先幫每位乘客算一個「生存分數」($z$)，公式如下：

$$z = w_1 x_1 + w_2 x_2 + \dots + b$$

**變數對照表 (Variable Explanation)：**
* **$z$ (Score)**：這是模型算出來的**線性分數**。分數越高，代表存活機率越大；分數越低（負數），代表越可能死亡。
* **$x$ (Features)**：這是乘客的**特徵資料**。
    * $x_1$：可能是「性別 (Sex)」
    * $x_2$：可能是「年齡 (Age)」
    * $x_3$：可能是「艙等 (Pclass)」
* **$w$ (Weights)**：這是**權重**，也就是模型學到的「經驗」。
    * 如果 $w_1$ (性別權重) 是很大的**負數**，代表如果是男性，分數 $z$ 會被扣很多分。
    * 如果 $w_3$ (艙等權重) 是正數，代表艙等越高，分數加越多。
* **$b$ (Bias)**：**偏差值**。代表基礎分數，就像是考試的基本分。

### 2.2 第二步：機率轉換 (Sigmoid Function)
算出來的 $z$ 可能是 100 或是 -500，這不像是「機率」。我們需要把它壓縮到 **0 ~ 1** 之間，這就是著名的 **Sigmoid 函數**：

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**變數對照表：**
* **$\sigma(z)$**：讀作 Sigma，這裡代表最終預測的 **生存機率 (Probability)**。範圍在 0% 到 100%。
* **$e$**：自然對數的底數 (約等於 2.718)。
* **$z$**：就是第一步算出來的那個「生存分數」。

**直觀理解：**
* 當分數 $z$ 很大時 (例如 10)，$e^{-10} 接近 0，結果接近 $\frac{1}{1} = 1$ (100% 存活)。
* 當分數 $z$ 很負時 (例如 -10)，$e^{10} 很大，分母變超大，結果接近 $0$ (0% 存活)。

---

## 3. 實驗結果：鐵達尼號數據 (The Titanic Experiment)
![Sigmoid Curve](https://github.com/ksharry/30-Days-Of-ML/blob/main/day4/pic/4-2.jpg?raw=true)
![Accuracy](https://github.com/ksharry/30-Days-Of-ML/blob/main/day4/pic/4-3.jpg?raw=true)

### 3.1 Sigmoid 曲線與決策邊界
意義： 這是**「可視化驗證」**。
解讀：
* **藍色 S 型線**： 就是 Image 1 的 Sigmoid 函數圖形。
* **橘色點點**： 每一位乘客。你可以看到，只要 $z$ 分數 (橫軸) 大於 0 的乘客，機率 (縱軸) 都在 0.5 以上，被模型判定為「生存」。分數小於 0 的，就滑向左下角的「死亡區」。

### 3.2 混淆矩陣 (Confusion Matrix)
意義： 這是「成績單」，看模型猜對了多少。
* **TP (True Positive):** 模型猜他活，他也真的活了。
* **FN (False Negative):** 模型猜他會死，結果他奇蹟生還

解讀：
* **深藍色區塊 (90, 55)**： 代表猜對了。模型猜死且真的死 (90人)，模型猜活且真的活 (55人)。
* **淺藍色區塊 (15, 19)**： 代表猜錯了。這些是模型需要檢討的地方（例如那 19 個被誤判為死亡的倖存者）。

模型跑完後，準確率大約落在 **81%**。

### 3.3 特徵重要性 (Feature Importance)
意義： 這是模型的「權重 ($w$) 排行榜」，告訴我們誰對 $z$ 分數影響最大。
解讀：
* **紅色長條 (Sex_male)**： 最長且向左（負值）。代表「男性」這個特徵會讓 $z$ 分數被扣最重，導致生存率大幅下降。
* **綠色長條 (Fare)**： 向右（正值）。代表票價付得越高，生存分數會稍微加一點分。

---

## 4. 深度分析：誰最容易活下來？

透過觀察程式跑出來的 **係數圖 (Coefficients)**，我們不僅能預測生死，還能理解「為什麼」：

1.  **性別 (Sex_male) - 決定性的權重**：
    *   權重約 **-2.5**。這是所有特徵中影響力最大的。
    *   這意味著，只要你是「男性」，你的 $z$ 分數會瞬間被扣掉 2.5 分。在 Sigmoid 函數上，這幾乎直接把你推向死亡區。這數據血淋淋地證實了當時 "Women and children first" 的逃生準則。

2.  **艙等 (Pclass) - 階級的殘酷**：
    *   權重約 **-0.9**。
    *   注意這裡是負相關。因為艙等數字越大 (1 -> 2 -> 3)，代表等級越低。
    *   數據告訴我們：住在三等艙的乘客，生存機率顯著低於頭等艙。這可能與逃生動線距離甲板較遠，或救援順序有關。

3.  **年齡 (Age) - 年輕就是本錢？**：
    *   權重呈現微幅負值。
    *   雖然我們常說「老弱婦孺」，但在統計上，年齡越大，生存機率確實有些微下降的趨勢。不過邏輯回歸只能抓到「線性關係」，如果真實情況是「小孩活、老人活、壯年死」這種 U 型關係，它可能就抓不準了。

---

## 5. 總結 (Conclusion)

今天我們學會了機器學習中最經典的分類器 —— **邏輯回歸 (Logistic Regression)**。

### 它的核心靈魂：
1.  **線性計分 ($z = wx + b$)**：像個嚴格的考官，根據你的特徵（性別、艙等）幫你打分數。
2.  **Sigmoid 激活** $$\sigma(z)$$：像個翻譯官，把分數翻譯成 0% ~ 100% 的機率。
3.  **可解釋性強**：這是它最大的優點。我們不只知道預測結果，還能透過權重 ($w$) 知道模型是根據什麼做判斷的（例如我們發現性別權重最大）。

### 下一步挑戰 (Next Step)：
邏輯回歸畫出的這條「生死界線」本質上是一條**直線 (Linear)**。
但如果世界不是線性的呢？如果倖存者像是「甜甜圈」一樣包圍著死亡者，直線切不開怎麼辦？

**Day 05 - K-近鄰演算法 (KNN)** 將登場。
我們將拋棄公式，改用「物以類聚」的直覺方式——看看你鄰居是活是死，來決定你的命運！我們將重新挑戰鐵達尼號，看看準確率能否突破！
