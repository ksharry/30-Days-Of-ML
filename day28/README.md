# Day 28: LSTM (Long Short-Term Memory) - 解決金魚腦

## 0. 歷史小故事/核心貢獻者:
RNN 雖然理論上很美好，但實務上因為 **梯度消失 (Vanishing Gradient)**，它記不住長期的資訊 (金魚腦)。
**Sepp Hochreiter** 和 **Jürgen Schmidhuber (1997)** 發明了 **LSTM**。
他們引入了 **「閘門 (Gates)」** 的機制，讓神經網路可以主動選擇 **「該忘記什麼」** 和 **「該記住什麼」**。這讓 AI 終於能讀懂長篇大論，或預測長期的股價趨勢。

## 1. 資料集來源
### 資料集來源：模擬股價資料 (Simulated Stock Data)
> 備註：與 Day 27 相同，但這次我們挑戰 **Look-back = 50 天** (RNN 通常超過 10~20 天就暈了)。

## 2. 原理
### 核心概念：為什麼 LSTM 記性好？
RNN 像是一個 **「傳話遊戲」**，話傳到第 100 個人一定會走樣。
LSTM 像是一條 **「行李輸送帶 (Conveyor Belt)」**。
*   資訊 (行李) 放在輸送帶上，可以一路從頭傳到尾，中間幾乎不會有損耗。
*   只有在經過特定的 **「閘門 (Gates)」** 時，我們才會對行李做操作 (拿走或放上)。

### 圖解：LSTM 的內部構造
![LSTM Concept](pic/28-2_LSTM_Concept.png)

#### 2.1 兩條核心動線 (The Two Highways)
LSTM 的運作可以看作是兩條線的交互作用：

1.  **頂部傳送帶 ($C_t$) - 長期記憶 (Long-term Memory)**
    *   這是 LSTM 最重要的發明！它像一條**高速公路**，貫穿整個時間軸。
    *   資訊在這條路上流動時，只有 **「乘法 (X)」** 和 **「加法 (+)」** 兩種操作，這讓資訊很難流失 (解決了梯度消失)。
    *   它負責把很久以前的記憶 (例如 50 天前的趨勢) 一路帶到今天。

2.  **底部工作區 ($h_t$) - 短期記憶 (Short-term Memory)**
    *   這是當下的輸出，也是要傳給下一個時間點的短期資訊。
    *   它會不斷地與 $C_t$ 互動：
        *   提供資訊來決定 $C_t$ 該忘記什麼。
        *   從 $C_t$ 提取資訊來決定當下該輸出什麼。

#### 2.2 三個閘門 (The Three Gates)
這三個門負責控制高速公路 ($C_t$) 的交流道：

1.  **遺忘門 (Forget Gate)** - `sigmoid` (黃色框)
    *   **位置**：圖中最左邊。
    *   **動作**：看著昨天的 $h_{t-1}$ 和今天的 $x_t$，決定要讓 $C_{t-1}$ **「乘上多少」** (粉紅 X)。
    *   **意義**：如果是 0 就全忘，1 就全記。(例如：趨勢改變了，忘掉舊的趨勢)。

2.  **輸入門 (Input Gate)** - `sigmoid` + `tanh` (中間兩個黃色框)
    *   **位置**：圖的中間。
    *   **動作**：計算出新的資訊，並 **「加到」** $C_t$ 上 (粉紅 +)。
    *   **意義**：把今天學到的新東西寫進長期記憶。(例如：發現新的上漲訊號)。

3.  **輸出門 (Output Gate)** - `sigmoid` (右邊黃色框)
    *   **位置**：圖的右邊。
    *   **動作**：看著 $h_{t-1}$ 和 $x_t$，決定要從 $C_t$ 裡 **「提取」** 什麼資訊變成 $h_t$。
    *   **意義**：雖然長期記憶裡有很多東西，但我只輸出現在需要的。(例如：預測明天的價格)。

#### 2.3 核心公式 (The Math)
雖然 LSTM 看起來很複雜，但其實就是一堆 **矩陣乘法** 和 **激活函數**：

*   **遺忘門 ($f_t$)**：`f_t = sigmoid(W_f * [h_{t-1}, x_t] + b_f)`
    *   決定舊記憶 $C_{t-1}$ 要保留多少 (0~1)。
*   **輸入門 ($i_t$)**：`i_t = sigmoid(W_i * [h_{t-1}, x_t] + b_i)`
    *   決定新資訊 $\tilde{C}_t$ 要存入多少 (0~1)。
*   **新記憶候選 ($\tilde{C}_t$)**：`C_tilde = tanh(W_C * [h_{t-1}, x_t] + b_C)`
    *   準備要存入的新資訊內容 (-1~1)。
*   **更新傳送帶 ($C_t$)**：`C_t = f_t * C_{t-1} + i_t * C_tilde`
    *   **舊的乘上遺忘係數 + 新的乘上輸入係數**。
*   **輸出門 ($o_t$)**：`o_t = sigmoid(W_o * [h_{t-1}, x_t] + b_o)`
    *   決定要輸出多少隱藏狀態。
*   **隱藏狀態 ($h_t$)**：`h_t = o_t * tanh(C_t)`
    *   最後輸出的結果。

> **參數說明**：
> *   $W_f, W_i, W_C, W_o$：這些就是模型要學習的 **權重 (Weights)**。
> *   $[h_{t-1}, x_t]$：把「昨天的隱藏狀態」和「今天的輸入」接在一起。

> **圖解補充：為什麼有 X 和 + ？**
> *   **X (乘法)**：代表 **「閘門控制」**。乘以 0 就是關掉 (不通過)，乘以 1 就是全開 (通過)。
> *   **+ (加法)**：代表 **「資訊疊加」**。把新資訊「加」到舊記憶上，而不是「替換」掉它。這讓 LSTM 能同時記住新舊資訊。

## 3. 實戰
### Python 程式碼實作
完整程式連結：[LSTM_Stock_Prediction.py](LSTM_Stock_Prediction.py)

```python
# 關鍵程式碼：建立 LSTM 模型

from tensorflow.keras.layers import LSTM, Dense

model = Sequential([
    # LSTM 層
    # units=50: 50 個 LSTM 單元 (比 RNN 複雜，通常需要多一點單元)
    # input_shape=(50, 1): 這次我們回看 50 天！
    LSTM(50, input_shape=(50, 1), activation='tanh'),
    
    # 輸出層
    Dense(1)
])
```

## 4. 模型評估與視覺化
### 預測結果展示
![Prediction Result](pic/28-1_Prediction_Result.png)
*   **觀察**：
    *   即使 Look-back 增加到 **50 天**，LSTM 依然能穩定訓練，沒有發生梯度消失。
    *   預測曲線 (紅線) 依然緊緊咬住真實股價 (灰線)。
    *   這證明了 LSTM 具備 **「長期記憶」** 的能力。

## 5. 戰略總結: RNN vs LSTM

| 特性 | Simple RNN | LSTM |
| :--- | :--- | :--- |
| **記憶力** | 短期 (金魚腦) | 長期 (大象腦) |
| **結構** | 簡單 (1 個 tanh 層) | 複雜 (4 個交互層 + 3 個門) |
| **訓練速度** | 快 | 慢 (參數多 4 倍) |
| **適用場景** | 短序列、簡單規律 | 長序列、複雜語意 (NLP)、股票 |
| **致命傷** | 梯度消失 | 運算量大 |

## 6. 總結
Day 28 我們學習了 **LSTM**。
*   它是時間序列領域的霸主 (直到 Transformer 出現之前)。
*   透過 **Cell State (傳送帶)** 和 **Gates (閘門)**，它完美解決了梯度消失問題。

下一章 (Day 29)，我們將進入 **模型部署 (Deployment)**。
模型訓練好之後，只能在自己的電腦跑是不夠的。我們要學習如何使用 **Streamlit**，快速把你的 AI 模型變成一個 **Web App**，讓全世界都能使用你的發明！
(我們將實作一個 AI 網頁應用程式)。
