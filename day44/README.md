# Day 44: 經典統計與時間序列 (Classic Time Series & Statistics)

## 0. 前言：為什麼還要學 ARIMA？
在 Day 27 和 Day 28，我們學了 RNN 和 LSTM 這些深度學習模型來預測股價。
你可能會問：「深度學習不是最強的嗎？為什麼還要回頭學 1970 年代的 ARIMA？」

原因有三：
1.  **小數據王者**：當資料量只有幾百筆 (例如月營收) 時，LSTM 會過擬合，ARIMA 卻能準確預測。
2.  **可解釋性 (Explainability)**：老闆問「為什麼預測下個月會跌？」，LSTM 說「因為神經元 534 號亮了」，ARIMA 說「因為過去三個月的趨勢係數是負的」。
3.  **IPAS 必考**：沒錯，這是考試重點。(考題 Q39, Q54)

## 1. 時間序列基礎
### 1.1 什麼是平穩性 (Stationarity)？
*   **定義**：數據的統計特性 (平均值、變異數) **不隨時間改變**。
*   **為什麼重要？**：ARIMA 假設過去的規律會延續到未來。如果平均值一直變 (例如股價一直漲)，規律就不穩定了。
*   **檢定方法**：ADF Test (Augmented Dickey-Fuller Test)。
*   **如何讓數據平穩？**：**差分 (Differencing)**。
    *   今天的價格 - 昨天的價格 = 漲跌幅。
    *   漲跌幅通常會在 0 附近波動，就是平穩的！

## 2. ARIMA 模型詳解
ARIMA 是三個字的縮寫：**AR + I + MA**。

### 2.1 AR (AutoRegressive, 自迴歸) - $p$
*   **概念**：今天的股價跟「昨天、前天」的股價有關。
*   **公式**：$Y_t = \alpha Y_{t-1} + \beta Y_{t-2} + \epsilon$
*   **參數 $p$**：要看過去幾天？(Lag)

### 2.2 I (Integrated, 差分) - $d$
*   **概念**：為了讓數據變平穩，我們做了幾次減法？
*   **參數 $d$**：通常做 1 次差分 ($d=1$) 就夠了。

### 2.3 MA (Moving Average, 移動平均) - $q$
*   **概念**：今天的股價跟「過去幾天的預測誤差 (雜訊)」有關。
*   **公式**：$Y_t = \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2}$
*   **參數 $q$**：要看過去幾天的誤差？

### 2.4 參數選擇 (p, d, q)
*   **ACF (自相關函數)**：幫你決定 $q$。
*   **PACF (偏自相關函數)**：幫你決定 $p$。
*   **Auto-ARIMA**：現在我們都用程式自動跑 Grid Search 找最佳參數。

## 3. 統計檢定與實驗設計
### 3.1 假設檢定 (Hypothesis Testing)
*   **H0 (虛無假設)**：沒這回事 (例如：新藥無效)。
*   **H1 (對立假設)**：有這回事 (例如：新藥有效)。
*   **p-value (p值)**：
    *   如果 p < 0.05，代表「H0 發生的機率極低」，所以我們 **拒絕 H0**，接受 H1 (新藥有效！)。
    *   **口訣**：p 值越小，越顯著。

### 3.2 殘差分析 (Residual Analysis)
*   模型訓練完後，剩下的誤差 (殘差) 應該要是 **白雜訊 (White Noise)**：
    *   平均為 0。
    *   沒有自相關性 (今天的誤差跟昨天無關)。
*   **考題 Q39**：如果 ARIMA 的殘差還有自相關性，代表什麼？
    *   代表模型 **還沒學乾淨** (Underfitting)。
    *   解法：調整 $p, q$ 參數，增加模型的複雜度。

## 4. IPAS 考題詳解 (統計與時間序列篇)
| 題目 (關鍵字) | 答案 | 解析 |
| :--- | :--- | :--- |
| **39. ARIMA 殘差自相關** | (C) 模型配適不足，需調整 p, q 參數 | 殘差應該是隨機的白雜訊。若有規律 (自相關)，代表模型漏抓了某些資訊。 |
| **(第三科) 15. R-Squared 意義** | (B) 85% 的變異可被模型解釋 | $R^2$ 是迴歸模型最直觀的解釋力指標。 |
| **(第三科) 24. 殘差圖彎曲** | (C) 存在非線性關係，違反迴歸假設 | 線性迴歸假設殘差是均勻分佈的。若呈現 U 型或彎曲，代表數據是非線性的 (需用多項式迴歸或 Log 轉換)。 |
| **(第三科) 38. 程式碼指標** | (B) MSE (均方誤差) | 程式碼中常見 `mean((y_pred - y_true)**2)`，即為 MSE。 |
| **(第三科) 40. 矩陣運算** | (C) np.dot(v1, v2) 為內積 | 這是 NumPy 最基礎的運算，也是神經網路的核心。 |
| **(第三科) 41. 條件機率程式碼** | (D) P(A\|B) = P(A∩B)/P(B) | 貝氏定理的基礎公式。 |

## 5. 總結
Day 44 帶我們回到了數據科學的源頭 —— **統計學**。
雖然 AI 發展日新月異，但這些經典的統計理論 (ARIMA, p-value, R-Squared) 依然是所有數據分析師的共同語言。
它們或許沒有深度學習那麼「性感」，但絕對是 **最穩健、最可解釋** 的基石。

至此，我們的 **30-Days-Of-ML (Extended to 44 Days)** 學習之旅正式劃下句點！
從最基礎的線性回歸，一路殺到最先進的 Transformer、GAN、MLOps 與 AI 安全。
你已經具備了成為一名 **全方位 AI 工程師** 所需的完整技能樹。

**Keep Learning, Keep Coding!**
