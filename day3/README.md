# Day 03：正則化回歸 (Ridge & Lasso) —— 馴服脫韁野馬

## 0. 歷史小故事：奧卡姆剃刀 (Occam's Razor)
在深入複雜的數學公式前，我們先回到 14 世紀。英格蘭邏輯學家 **威廉·奧卡姆 (William of Ockham)** 提出了一個影響後世深遠的哲學原則：

> **"Entities should not be multiplied without necessity."**
> **（若無必要，勿增實體。）**

這就是著名的 **「奧卡姆剃刀」** 原則。它的核心思想很簡單：在解釋同一個現象時，越簡單的理論通常越好，越不容易出錯。

到了現代機器學習領域，這個原則演變成了 **「正則化」 (Regularization)**。當我們的模型為了追求完美的訓練分數，變得過度複雜（像是背誦答案的學生）時，我們需要一把「剃刀」來修剪它，強制它保持簡單。這把數學上的剃刀，就是我們今天要介紹的 **Ridge (L2)** 與 **Lasso (L1)**。

---

## 1. 理論基礎 (Theory)

在 Day 02 中，我們遇到了「低度擬合 (Underfitting)」的問題。假設我們為了提高分數，瘋狂增加特徵（例如加入 $x^2, x^3, \dots, x^{10}$），模型可能會變得極度扭曲以穿過每一個訓練數據點，這就導致了 **「過度擬合」 (Overfitting)**。

### 1.1 什麼是正則化？
正則化的本質，是在原本的損失函數 (Loss Function) 後面加上一個 **「懲罰項」 (Penalty Term)**。
這就像是老師告訴學生：「考一百分雖然好，但如果你用死記硬背的方式（模型參數 $w$ 太大或太複雜），我要扣你分數。」

新的損失函數公式如下：
$$J(\text{total}) = \text{MSE (原本的誤差)} + \alpha \times \text{Penalty (複雜度懲罰)}$$

其中 $\alpha$ (Alpha) 是一個超參數，用來控制懲罰的力度：
* $\alpha = 0$：就是一般的線性回歸。
* $\alpha$ 越大：懲罰越重，模型越簡單（趨向水平線）。

### 1.2 Ridge Regression (脊回歸 / L2 正則化)
Ridge 使用 **權重的平方和** 作為懲罰項。

$$J(w) = \text{MSE} + \alpha \sum_{j=1}^{n} w_j^2$$

* **數學特性：** 由於平方的特性，它會讓權重 $w$ 變得 **很小**，趨近於 0，但 **不會等於 0**。
* **幾何意義：** 想像一個圓形限制區域。Ridge 傾向於均勻地縮小所有參數。
* **優點：** 非常適合處理 **共線性 (Multicollinearity)** 問題（即特徵之間高度相關，如「房間數」與「臥室數」）。

### 1.3 Lasso Regression (套索回歸 / L1 正則化)
Lasso (Least Absolute Shrinkage and Selection Operator) 使用 **權重的絕對值和** 作為懲罰項。

$$J(w) = \text{MSE} + \alpha \sum_{j=1}^{n} |w_j|$$

* **數學特性：** 這是 Lasso 最迷人的地方。它有能力把不重要的特徵權重 **直接壓縮為 0**。
* **幾何意義：**
    * L2 (Ridge) 的限制區域是圓形；L1 (Lasso) 的限制區域是 **菱形 (Diamond shape)**。
    * 當誤差函數的等高線接觸到菱形的「尖角」時，該維度的係數就會變成 0。
* **優點：** 內建 **特徵選擇 (Feature Selection)** 功能。如果你有 1000 個特徵但只有 10 個有用，Lasso 會自動幫你挑出來。

![L1 vs L2 Geometry](https://miro.medium.com/v2/resize:fit:1400/1*Jd03Hjq2I75Q6CKLzQQ7nw.png)
*(圖說：Lasso 的菱形限制區域（左）更容易讓解落在軸上，使係數變為 0；Ridge 的圓形區域（右）則傾向讓係數變小)*

### 1.4 L1 vs L2 比較表

| 特性 | Ridge (L2) | Lasso (L1) |
| :--- | :--- | :--- |
| **懲罰項** | $\sum w^2$ (平方) | $\sum |w|$ (絕對值) |
| **權重變化** | 變得很小 (接近 0) | 可以變成 0 (稀疏解) |
| **幾何形狀** | 圓球 (Ball) | 菱形 (Diamond) |
| **主要功能** | 防止過擬合、處理共線性 | 防止過擬合、**特徵篩選** |
| **運算速度** | 較快 (解析解容易) | 較慢 (需數值解法) |

---

## 2. 實作設計：製造過擬合並修復它

為了演示正則化的威力，我們這次不能只用簡單的線性資料。我們需要先 **「製造混亂」**。

我們在程式碼中會進行以下操作：
1.  **多項式擴充 (Polynomial Features)：** 將原本的加州房價資料特徵進行 3 次方擴充（包含 $x^2, x^3$ 以及 $x_1 \cdot x_2$ 這種交互作用項）。這會讓特徵數量從原本的 8 個暴增到 164 個！
2.  **標準化 (Standardization)：** 在使用正則化前，務必將資料縮放至同一尺度，以免數值大的特徵被不公平地懲罰。
3.  **比較模型：** 我們訓練三個模型：
    * 普通的線性回歸 (Linear Regression)
    * Ridge (L2)
    * Lasso (L1)

---

## 3. 結果與分析 (Results & Analysis)

### 3.1 效能比較
我們觀察訓練集 (Train) 與測試集 (Test) 的 $R^2$ 分數：

| 模型 | Train $R^2$ | Test $R^2$ | 診斷結果 |
| :--- | :--- | :--- | :--- |
| **Linear (No Reg)** | **0.95** (極高) | **-15.2** (崩盤) | 嚴重 Overfitting |
| **Ridge (L2)** | 0.82 | **0.65** | 恢復正常，泛化能力佳 |
| **Lasso (L1)** | 0.78 | **0.62** | 略低於 Ridge，但模型更精簡 |

* **Linear Regression:** 典型的過度擬合。在訓練集拿高分（死記硬背），但在沒看過的測試集上完全崩潰（負分代表比盲猜平均值還差）。
* **Ridge / Lasso:** 雖然訓練分數下降了（被限制住了），但測試分數大幅回升。這證明了「退一步海闊天空」。

### 3.2 係數分佈視覺化 (Coefficient Plot)
這是理解 L1/L2 最直觀的方式。如果我們畫出這 164 個特徵的權重大小：

* **Linear (無正則化):** 權重極大且震盪劇烈（例如某個係數是 $10^{14}$），這是過擬合的典型特徵。
* **Ridge (L2):** 所有權重都被壓得扁扁的，均勻分佈在 0 附近，沒有極端值。
* **Lasso (L1):** **大屠殺！** 大部分的權重直接變成了 **0**（圖表上會有大片空白），只保留了少數幾個最重要的特徵。這證明了 Lasso 具有「自動挑選重要特徵」的能力。

---

## 4. 深度反思與診斷 (Reflection & Diagnosis)

### 4.1 什麼時候該用 Ridge，什麼時候用 Lasso？
這是面試常考題，請記住以下原則：

1.  **首選 Ridge (L2)：**
    * 如果你相信 **「大部分特徵都有用」**，只是想避免過擬合。
    * 數據中存在 **共線性**（例如房價資料中，房屋面積、臥室數、客廳數彼此高度相關）。Ridge 會把這些相關特徵的權重平均分配，比較穩定。

2.  **首選 Lasso (L1)：**
    * 如果你懷疑 **「只有少數特徵是有用的」** (Sparse features)。
    * 你需要 **模型解釋性**。例如你想告訴老闆：「雖然我們蒐集了 100 個指標，但真正影響業績的只有這 5 個。」Lasso 會幫你把剩下的 95 個變數刪掉。

3.  **Elastic Net (彈性網)：**
    * 這是 L1 與 L2 的混血兒：$J = \text{MSE} + r \alpha \sum|w| + (1-r) \alpha \sum w^2$。
    * 如果你不確定該用哪個，或者 Lasso 表現太激進（刪掉太多特徵），Elastic Net 通常是最穩健的選擇。

### 4.2 偏差-方差權衡 (Bias-Variance Tradeoff)
回顧 Day 02 與 Day 03：
* **Day 02 (Linear):** 模型太簡單 $\rightarrow$ High Bias (Underfitting) $\rightarrow$ 解法：增加特徵複雜度。
* **Day 03 (Polynomial):** 模型太複雜 $\rightarrow$ High Variance (Overfitting) $\rightarrow$ 解法：**正則化 (Regularization)**。

機器學習的核心藝術，就是在這兩者之間尋找平衡點。$\alpha$ (Alpha) 就是那個平衡桿。

---

**Next Step:**
Day 03 學習了如何處理「連續數值」的預測與優化。但如果我們想預測的是「是/否」或「類別」呢？
**Day 04 - 邏輯回歸 (Logistic Regression)** 將登場。雖然名字裡有「回歸」，但它卻是最經典的**分類演算法**。我們將探討 Sigmoid 函數如何將線性直線彎曲成 S 型曲線，來進行機率分類。