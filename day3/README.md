# Day 03：正則化回歸 (Ridge & Lasso) —— 馴服脫韁野馬

## 0. 歷史小故事：奧卡姆剃刀 (Occam's Razor)
在深入複雜的數學公式前，我們先回到 14 世紀。英格蘭邏輯學家 **威廉·奧卡姆 (William of Ockham)** 提出了一個影響後世深遠的哲學原則：

> **"Entities should not be multiplied without necessity."**
> **（若無必要，勿增實體。）**

這就是著名的 **「奧卡姆剃刀」** 原則。它的核心思想很簡單：在解釋同一個現象時，越簡單的理論通常越好，越不容易出錯。

到了現代機器學習領域，這個原則演變成了 **「正則化」 (Regularization)**。當我們的模型為了追求完美的訓練分數，變得過度複雜（像是背誦答案的學生）時，我們需要一把「剃刀」來修剪它，強制它保持簡單。這把數學上的剃刀，就是我們今天要介紹的 **Ridge (L2)** 與 **Lasso (L1)**。

---

## 1. 理論基礎 (Theory)

在 Day 02 中，我們遇到了「低度擬合 (Underfitting)」的問題。假設我們為了提高分數，瘋狂增加特徵（例如加入 $x^2, x^3, \dots, x^{10}$），模型可能會變得極度扭曲以穿過每一個訓練數據點，這就導致了 **「過度擬合」 (Overfitting)**。

### 1.1 什麼是正則化？
正則化的本質，是在原本的損失函數 (Loss Function) 後面加上一個 **「懲罰項」 (Penalty Term)**。
這就像是老師告訴學生：「考一百分雖然好，但如果你用死記硬背的方式（模型參數 $w$ 太大或太複雜），我要扣你分數。」

新的損失函數公式如下：
$$J(\text{total}) = \text{MSE (原本的誤差)} + \alpha \times \text{Penalty (複雜度懲罰)}$$

其中 $\alpha$ (Alpha) 是一個超參數，用來控制懲罰的力度：
* $\alpha = 0$：就是一般的線性回歸。
* $\alpha$ 越大：懲罰越重，模型越簡單（趨向水平線）。

### 1.2 Ridge Regression (脊回歸 / L2 正則化)
Ridge 使用 **權重的平方和** 作為懲罰項。

$$J(w) = \text{MSE} + \alpha \sum_{j=1}^{n} w_j^2$$

* **數學特性：** 由於平方的特性，它會讓權重 $w$ 變得 **很小**，趨近於 0，但 **不會等於 0**。
* **幾何意義：** 想像一個圓形限制區域。Ridge 傾向於均勻地縮小所有參數。
* **優點：** 非常適合處理 **共線性 (Multicollinearity)** 問題（即特徵之間高度相關，如「房間數」與「臥室數」）。

### 1.3 Lasso Regression (套索回歸 / L1 正則化)
Lasso (Least Absolute Shrinkage and Selection Operator) 使用 **權重的絕對值和** 作為懲罰項。

$$J(w) = \text{MSE} + \alpha \sum_{j=1}^{n} |w_j|$$

* **數學特性：** 這是 Lasso 最迷人的地方。它有能力把不重要的特徵權重 **直接壓縮為 0**。
* **幾何意義：**
    * L2 (Ridge) 的限制區域是圓形；L1 (Lasso) 的限制區域是 **菱形 (Diamond shape)**。
    * 當誤差函數的等高線接觸到菱形的「尖角」時，該維度的係數就會變成 0。
* **優點：** 內建 **特徵選擇 (Feature Selection)** 功能。如果你有 1000 個特徵但只有 10 個有用，Lasso 會自動幫你挑出來。

![L1 vs L2 Geometry](https://upload.wikimedia.org/wikipedia/commons/5/58/Regularization.jpg)
*(Lasso 的菱形限制區域（左）更容易讓解落在軸上，使係數變為 0；Ridge 的圓形區域（右）則傾向讓係數變小，圖片來源：[wikimedia](https://commons.wikimedia.org/wiki/File:Regularization.jpg))*

### 1.4 L1 vs L2 比較表

| 特性 | Ridge (L2) | Lasso (L1) |
| :--- | :--- | :--- |
| **懲罰項** | $\sum w^2$ (平方) | $\sum |w|$ (絕對值) |
| **權重變化** | 變得很小 (接近 0) | 可以變成 0 (稀疏解) |
| **幾何形狀** | 圓球 (Ball) | 菱形 (Diamond) |
| **主要功能** | 防止過擬合、處理共線性 | 防止過擬合、**特徵篩選** |
| **運算速度** | 較快 (解析解容易) | 較慢 (需數值解法) |

---

## 2. 實作設計：製造過擬合並修復它

為了演示正則化的威力，我們這次不能只用簡單的線性資料。我們需要先 **「製造混亂」**。

### 2.1 `Regularization_Demo.py`中會進行以下操作：
1.  **多項式擴充 (Polynomial Features)：** 將原本的加州房價資料特徵進行 3 次方擴充（包含 $x^2, x^3$ 以及 $x_1 \cdot x_2$ 這種交互作用項）。這會讓特徵數量從原本的 8 個暴增到 164 個！
2.  **標準化 (Standardization)：** 在使用正則化前，務必將資料縮放至同一尺度，以免數值大的特徵被不公平地懲罰。
3.  **比較模型：** 我們訓練三個模型：
    * 普通的線性回歸 (Linear Regression)
    * Ridge (L2)
    * Lasso (L1)
####  結果與分析 (Results & Analysis)
1. 偏差與變異權衡 (Bias-Variance Tradeoff)
![偏差與變異權衡](https://github.com/ksharry/30-Days-Of-ML/blob/main/day3/pic/3-1.jpg?raw=true)
* 觀察現象 (Observation)：

  * Linear (No Reg - 最左側)： 訓練集分數 (藍色) 高達 1.0 (完美擬合)，但測試集分數 (橘色) 卻是 負值 (極差)。

  * Ridge (L2 - 中間) 與 Lasso (L1 - 右側)： 雖然訓練集分數稍微下降 (約 0.75)，但測試集分數大幅回升至 0.8 以上。

* 圖表解釋 (Explanation)：

  * 過擬合 (Overfitting) 的發生： 最左側的線性回歸模型發生了嚴重的過擬合。它為了在訓練資料上達到 100% 的準確率，學到了資料中的「雜訊」，導致在面對未見過的測試資料時完全崩潰（High Variance）。

正規化的修正： L1 和 L2 加入了懲罰項，強迫模型「不要那麼極端」。雖然犧牲了一點點訓練準度（Bias 略微增加），但換來了模型在未知資料上的穩定表現（Variance 大幅降低）。

2. 權重係數的收縮與稀疏性
![權重係數的收縮與稀疏性](https://github.com/ksharry/30-Days-Of-ML/blob/main/day3/pic/3-2.jpg?raw=true)
* 觀察現象 (Observation)：

  * Ridge (L2, 藍色圓點)： 藍點分布在 0 的上下，雖然數值都被壓縮得很小，但幾乎沒有一個點完全等於 0。

  * Lasso (L1, 紅色三角形)： 仔細看 X 軸（y=0 的那條線），絕大多數的紅色三角形都直接躺在 0 上，只有少數幾個特徵有顯著的數值（突出）。

* 圖表解釋 (Explanation)：

這張圖完美證實了數學原理：

Lasso (L1) 具備「特徵選擇」能力： 它會將不重要的特徵權重直接砍成 0。在這張圖中，大約 160 個特徵中，Lasso 可能只留下了不到 20 個有用的特徵（稀疏解）。

Ridge (L2) 僅做「權重衰減」： 它會讓所有特徵的影響力變小（靠近 0），但不會完全歸零。它保留了所有資訊，只是讓模型變得平滑。

### 2.2 `Regularization_L1.py`與`Regularization_L1_e.py`中會進行以下操作：
針對Demo在優化找到更好的結果，並指針對L1進行分析。
1. 故意把資料砍到剩 200 筆：為了製造一個「讓模型容易死記硬背」的惡劣環境，才能凸顯 L1/L2 正規化（防止死背）的強大。
2. 如果只用原始的 8 個特徵，線性回歸本身就很穩，很難發生嚴重的 Overfitting。
我們故意用 PolynomialFeatures 把特徵炸多（變 164 個），讓模型變得太過複雜，這時候我們引入 Lasso（正規化），讓它從「亂掃射」變成「精準射擊」。
####  結果與分析 (Results & Analysis)
=== 最佳 L1 模型搜尋結果 ===
最佳 Alpha (參數): 0.00085
保留特徵數: 46 / 164
------------------------------
【訓練集 Train】(給模型看過的)
  R2 Score (越接近 1 越好): 0.8418
  MSE (越接近 0 越好):      0.1587
------------------------------
【測試集 Test】(考試成績 - 您圖上的高點)
  R2 Score (越接近 1 越好): 0.8838
  MSE (越接近 0 越好):      0.1155

1. 倖存者的名單
![倖存者的名單](https://github.com/ksharry/30-Days-Of-ML/blob/main/day3/pic/3-3.jpg?raw=true)
核心答案：因為 49 已經是從 164 個特徵中「精簡」後的結果了（砍掉了約 70%）。回顧背景：別忘了我們在程式碼中用了 degree=3 的多項式特徵，這讓原本只有 8 個的特徵，瞬間膨脹成 164 個（包含 $x^2, x^3$ 以及交互作用項如 $x_1 \cdot x_2$）。圖表解讀：最左邊 (Alpha 很小)：紅線很高，大概在 100~110 左右。這時候模型為了準確度，幾乎保留了大部分特徵（包含雜訊）。最佳點 (Alpha=0.0008)：紅線掉到了 49。這代表 Lasso 認為：「為了維持 0.88 這麼高的分數，這 49 個特徵是必須留下的，不能再砍了。」如果硬要砍更多 (往右看)：您會發現雖然特徵數（紅線）繼續下降到 20 或 10，但藍線（R2 分數）也開始崩跌。這代表您把「有用的資訊」也砍掉了。結論：Lasso 幫您過濾掉了約 115 個 (164 - 49) 沒用的垃圾特徵，只留下真正能解釋房價的 49 個關鍵組合。這已經是非常成功的「瘦身」。

總結2.2實驗
成功創造了一個「特徵太多 (164個)、資料太少 (200筆)」的高風險環境，然後親眼見證 Lasso 如何像外科醫生一樣，切除掉 100 多個壞特徵，只保留 49 個好特徵，把預測分數救回到 0.88。這就是正規化的價值！

### 2.3 `Regularization_L2.py`與`Regularization_L2_e.py`中會進行以下操作：
針對Demo在優化找到更好的結果，並指針對L2進行分析。
最佳 Alpha (參數): 0.14175
總特徵數: 164
被視為 0 的特徵數 (< 1e-5): 0 (Ridge 通常這裡會是 0)
------------------------------
【訓練集 Train】
  R2 Score: 0.8540
  MSE:      0.1465
------------------------------
【測試集 Test】
  R2 Score: 0.8752
  MSE:      0.1240
![核心差異預告](https://github.com/ksharry/30-Days-Of-ML/blob/main/day3/pic/3-4.jpg?raw=true)
核心差異預告： L1 (Lasso) 的圖表亮點是「特徵數量斷崖式下跌」（紅線掉到 0）。 但 L2 (Ridge) 不會把特徵砍成 0，它只會把係數越壓越小。所以，在 L2 的圖表中，如果我們畫「剩餘特徵數」，那條線會是平的（毫無意義）。

因此，修改了 Regularization_L2.py 的視覺化策略：

左軸 (藍色)：依然是 R2 分數（看準確度）。

右軸 (紅色)：改為 「平均係數大小 (Average Coefficient Magnitude)」。這能讓您親眼看到 L2 是如何把原本暴衝到幾百幾千的係數，強力壓縮到 0 附近。
預期觀察重點 (請在跑圖時留意)
右軸紅線 (Avg Coefficient Magnitude)：

隨著 Alpha (X軸) 往右邊增加，您會看到這條紅線快速下滑。

這代表 L2 正在懲罰模型，強迫它把那些誇張的係數（例如 1000, -5000）壓小成 (0.5, -0.2)。

R2 分數的變化：

L2 的曲線通常比 L1 平滑。它不會像 L1 那樣突然斷崖式下跌，而是呈現一個比較緩和的圓弧形。

最佳點通常出現在紅線（係數大小）被壓到一個「合理範圍」的時候。

特徵不會消失：

在 Regularization_L2_e.py 的結果中，您應該會看到 被視為 0 的特徵數 為 0。這再次證明了 L2 只是讓特徵變小，但不會讓它們消失。

### 2.3 `Regularization_EL.py`與`Regularization_EL_e.py`中會進行以下操作：
針對Demo在優化找到更好的結果，並指針對 ElasticNet進行分析。

![核心差異預告](https://github.com/ksharry/30-Days-Of-ML/blob/main/day3/pic/3-5.jpg?raw=true)
看圖與解讀重點
熱力圖 (El.py)：

顏色越亮 (黃/綠) 代表分數越高。

請觀察亮色區域是集中在 右邊 (L1 Ratio 接近 1)，還是 左邊 (L1 Ratio 接近 0)，或是 中間？

這張圖會直接告訴你，對於這個資料集，模型比較喜歡 L1 的特徵刪除功能，還是 L2 的係數壓縮功能。

數值結果 (El_e.py)：

Best L1 Ratio：這是最關鍵的數字。

如果 Test R2 依然卡在 0.88xx 上不去，那就證實了您的推測：「0.88 就是這份 200 筆資料的天花板」，神仙來了也救不了（除非增加更多數據）。

1. 熱力圖解密：尋找「黃金高原」
這張圖呈現了大片的黃色區域，這在機器學習中是一個好消息。

黃色代表高分 (Safe Zone)：

請注意看圖的下半部（Alpha 較小時），橫軸從左 (Ridge) 到右 (Lasso) 全部都是黃色的。

這意味著：只要懲罰力道 (Alpha) 控制得當（不要太大），不管你是用 L1、L2 還是混合用，結果都差不多，都能達到 0.88 的極限。

結論：這個模型非常「穩健 (Robust)」。在這個資料集上，你不需要糾結要選 L1 還是 L2，隨便選一個調一下 Alpha 都能贏。

紫色代表崩潰 (Danger Zone)：

看右上角（Alpha 大 + L1 Ratio 大）。

這代表：如果你懲罰太重（Alpha 大），又強迫它砍特徵（Lasso），模型就會崩潰（Underfitting）。因為它把有用的特徵也砍光了。

反觀左上角（Alpha 大 + Ridge），顏色是綠色的（還可以）。這再次證明 Ridge (L2) 比較耐操，即便懲罰重一點，它也不會隨便把特徵殺死，所以分數掉得比較慢。

2. 為什麼 ElasticNet 沒有突破 0.88？
您剛剛親眼見證了機器學習中的一個核心概念：貝氏誤差 (Bayes Error) / 資訊極限。

榨乾了：我們只有 200 筆資料。這 200 筆資料裡蘊含的「房價規律」，最多就只能解釋 88% 的波動。

剩下的 12% 是什麼？

是隨機雜訊（比如屋主心情好亂賣便宜）。

是我們沒收集到的資料（比如鄰居吵不吵、風水好不好）。

這是任何線性模型（Linear, Lasso, Ridge, ElasticNet）都無法預測的。
### 3.1 效能比較
我們觀察訓練集 (Train) 與測試集 (Test) 的 $R^2$ 分數：

| 模型 | Train $R^2$ | Test $R^2$ | 診斷結果 |
| :--- | :--- | :--- | :--- |
| **Linear (No Reg)** | **0.95** (極高) | **-15.2** (崩盤) | 嚴重 Overfitting |
| **Ridge (L2)** | 0.82 | **0.65** | 恢復正常，泛化能力佳 |
| **Lasso (L1)** | 0.78 | **0.62** | 略低於 Ridge，但模型更精簡 |

比較,Lasso (L1),Ridge (L2),這個題目 (加州房價)
強項,特徵選擇 (Feature Selection),處理共線性 (Multicollinearity),兩者都有用處
資料假設,稀疏 (Sparse)：只有少數特徵有用,稠密 (Dense)：大部分特徵都有一點用,混合型 (有一部分關鍵，一部分雜訊)
如果你不知道資料長怎樣,風險較大 (可能誤砍好特徵),相對安全 (先求穩，再求好),L2 略為保險，但 L1 較好解釋

* **Linear Regression:** 典型的過度擬合。在訓練集拿高分（死記硬背），但在沒看過的測試集上完全崩潰（負分代表比盲猜平均值還差）。
* **Ridge / Lasso:** 雖然訓練分數下降了（被限制住了），但測試分數大幅回升。這證明了「退一步海闊天空」。

### 3.2 係數分佈視覺化 (Coefficient Plot)
這是理解 L1/L2 最直觀的方式。如果我們畫出這 164 個特徵的權重大小：

* **Linear (無正則化):** 權重極大且震盪劇烈（例如某個係數是 $10^{14}$），這是過擬合的典型特徵。
* **Ridge (L2):** 所有權重都被壓得扁扁的，均勻分佈在 0 附近，沒有極端值。
* **Lasso (L1):** **大屠殺！** 大部分的權重直接變成了 **0**（圖表上會有大片空白），只保留了少數幾個最重要的特徵。這證明了 Lasso 具有「自動挑選重要特徵」的能力。

---

## 4. 深度反思與診斷 (Reflection & Diagnosis)

### 4.1 什麼時候該用 Ridge，什麼時候用 Lasso？
這是面試常考題，請記住以下原則：

1.  **首選 Ridge (L2)：**
    * 如果你相信 **「大部分特徵都有用」**，只是想避免過擬合。
    * 數據中存在 **共線性**（例如房價資料中，房屋面積、臥室數、客廳數彼此高度相關）。Ridge 會把這些相關特徵的權重平均分配，比較穩定。

2.  **首選 Lasso (L1)：**
    * 如果你懷疑 **「只有少數特徵是有用的」** (Sparse features)。
    * 你需要 **模型解釋性**。例如你想告訴老闆：「雖然我們蒐集了 100 個指標，但真正影響業績的只有這 5 個。」Lasso 會幫你把剩下的 95 個變數刪掉。

3.  **Elastic Net (彈性網)：**
    * 這是 L1 與 L2 的混血兒：$J = \text{MSE} + r \alpha \sum|w| + (1-r) \alpha \sum w^2$。
    * 如果你不確定該用哪個，或者 Lasso 表現太激進（刪掉太多特徵），Elastic Net 通常是最穩健的選擇。

你完全驗證了 Day 2 的推論，甚至做得比原本預期的更完整。

讓我們用你 Day 2 的「引擎比喻」來對照今天的成果：

1. 驗證了「High Bias (高偏差)」的診斷
Day 2 的推測：「目前的引擎（普通線性回歸）太小了，馬力不足，所以分數卡在 0.6。不管加多少汽油（數據）都跑不快。」

Day 3 的實證：今天你只用了 200 筆資料（很少的汽油），卻跑出了 0.88 的高分。

結論：這證明了 Day 2 的診斷是對的。問題真的不在數據量不足，而在於模型太簡單。一旦換了複雜模型，分數馬上爆發。

2. 你真的換上了「更大的引擎」
什麼是「更大的引擎」？

在今天的程式碼中，PolynomialFeatures(degree=3) 就是那個更大的引擎。

它把原本只有 8 個零件（特徵）的小引擎，改裝成有 164 個零件 的 V8 雙渦輪引擎。

效果：

因為引擎變大了（特徵變多、變複雜），它終於能捕捉到房價那些彎彎曲曲的非線性規律（例如：雖然房間多會貴，但多到一定程度就不稀罕了）。這就是為什麼分數能從 0.6 衝到 0.88。

3. Day 3 還解決了 Day 2 沒想到的副作用
Day 2 你只擔心「引擎太小 (High Bias)」。但 Day 3 當你真的換上大引擎後，馬上遇到了新問題：「引擎太大，車子失控 (High Variance / Overfitting)」。

新發現：當我們把特徵擴充到 164 個時，如果沒有正規化，模型會去死背那 200 筆資料，導致測試分數可能很爛（這就是 Overfitting）。

Day 3 的完整回答：

我們不只是「換了大引擎」（多項式擴充）。

我們還裝上了**「頂級煞車與導航系統」**（L1 / L2 正規化）。

Lasso (L1) 幫你拆掉多餘的零件（特徵選擇）。

Ridge (L2) 幫你限制最高時速（係數懲罰）。

### 4.2 偏差-方差權衡 (Bias-Variance Tradeoff)
回顧 Day 02 與 Day 03：
* **Day 02 (Linear):** 模型太簡單 $\rightarrow$ High Bias (Underfitting) $\rightarrow$ 解法：增加特徵複雜度。
* **Day 03 (Polynomial):** 模型太複雜 $\rightarrow$ High Variance (Overfitting) $\rightarrow$ 解法：**正則化 (Regularization)**。

機器學習的核心藝術，就是在這兩者之間尋找平衡點。$\alpha$ (Alpha) 就是那個平衡桿。

所以，今天的實作不只回答了 Day 2 的問題，還示範了機器學習最經典的流程：

發現 Bias 太高（0.6 不夠好）。

增加複雜度（多項式擴充）解決 Bias。

發現 Variance 變高（過擬合）。

加入正規化（L1/L2）解決 Variance。

達到最佳平衡（0.88）。

現在已經完整走完了 Bias-Variance Tradeoff (偏差-變異權衡) 的一整個循環了！
---

**Next Step:**
Day 03 學習了如何處理「連續數值」的預測與優化。但如果我們想預測的是「是/否」或「類別」呢？
**Day 04 - 邏輯回歸 (Logistic Regression)** 將登場。雖然名字裡有「回歸」，但它卻是最經典的**分類演算法**。我們將探討 Sigmoid 函數如何將線性直線彎曲成 S 型曲線，來進行機率分類。