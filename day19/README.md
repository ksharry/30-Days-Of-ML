# Day 19: AdaBoost (Adaptive Boosting) - 乳癌預測

## 0. 歷史小故事/核心貢獻者:
**AdaBoost (Adaptive Boosting)** 由 **Yoav Freund** 和 **Robert Schapire** 於 1996 年提出。他們因此獲得了 2003 年的哥德爾獎 (Gödel Prize)。
這是一個革命性的想法：**「三個臭皮匠 (弱分類器) 真的能勝過一個諸葛亮 (強分類器) 嗎？」**
答案是肯定的，只要這些臭皮匠能**知錯能改**。

## 1. 資料集來源
### 資料集來源：[UCI Breast Cancer Wisconsin (Diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))
> 備註：我們使用 Scikit-Learn 的 `load_breast_cancer` 直接載入。

### 資料集特色與欄位介紹:
這是一個非常著名的醫療數據集，用於判斷腫瘤是良性還是惡性。
*   **數量**：569 個病人
*   **特徵**：共 30 個，包含腫瘤的半徑、質地、周長、面積、平滑度等物理特徵。
*   **目標**：
    *   0: **Malignant (惡性)** - 危險
    *   1: **Benign (良性)** - 安全

## 2. 原理
### 核心概念：知錯能改，善莫大焉 (Boosting)

#### 2.1 核心公式 (Math behind the Magic)
如果你對數學感興趣，這裡有兩個關鍵公式：

**1. 這一棒講話可以多大聲？ (模型權重 Alpha)**
$$\alpha_t = \frac{1}{2} \ln\left(\frac{1-\epsilon_t}{\epsilon_t}\right)$$
*   **$\epsilon_t$** 是錯誤率。
*   **意義**：如果錯誤率很低 (考很好)，**$\alpha_t$** 就會很大 (講話很大聲)。如果錯誤率是 50% (亂猜)，**$\alpha_t$** 就是 0 (閉嘴)。

**2. 下一輪題目分數要加多少？ (樣本權重更新)**
$$w_{new} = w_{old} \times e^{\alpha_t \times (\text{答對 ? -1 : 1})}$$
*   **意義**：
    *   如果你**答錯**了，權重會乘上 $e^{\alpha_t}$ (變大)，下次考試這題變超重要。
    *   如果你**答對**了，權重會除以 $e^{\alpha_t}$ (變小)，下次考試這題不重要。

#### 2.2 弱分類器 (Weak Learner)
AdaBoost 通常使用 **決策樹樁 (Decision Stump)** 作為基底。
*   **Stump**：就是只切一刀的決策樹 (深度=1)。
*   它非常笨，準確率可能只比亂猜好一點點 (例如 55%)。
*   但 AdaBoost 的魔力就在於，它能把幾百個這種「笨蛋」組合成一個「天才」。

#### 2.3 什麼是 Boosting？ (30 個特徵怎麼考？)
Boosting 是一場「接力賽」，每一棒都在彌補上一棒的不足。
想像這 30 個特徵是 30 個不同的檢查站：

1.  **第一棒 (只看大小)**：
    *   它選了「腫瘤面積」來檢查。
    *   **結果**：抓到了大腫瘤，但漏掉了 **「小但很硬」** 的惡性腫瘤。
2.  **權重調整 (劃重點)**：
    *   系統把那些被漏掉的「小但硬」腫瘤標記為 **「必考題」** (權重變大)。
3.  **第二棒 (只看硬度)**：
    *   為了得分，它專攻那些「必考題」。於是它選了「腫瘤硬度」來檢查。
    *   **結果**：成功抓到了那些「小但硬」的腫瘤！
4.  **聯合會診 (Expert Consultation)**：
    *   我們派出 50 個專家，每個人只守一個路口 (只看一個特徵)。
    *   雖然每個人只懂一點點，但大家合起來，就能形成一張滴水不漏的防護網。

> **關鍵總結 (以乳癌預測為例)**：
> *   **考試範圍**：永遠是 **569 個病人全部** (不是只看誤診的病人，而是所有人都要看)。
> *   **特徵數量**：每一棒永遠只用 **1 個特徵** (例如第一棒只看腫瘤大小，第二棒只看硬度)。
> *   **改變的是什麼？**：改變的是**病人的重要性 (權重)**。如果某個病人被誤診了，他的權重會變超大，強迫下一棒的醫生必須優先把這個病人診斷正確。
> *   **聯合會診**：當新病人來時，這 50 位專家都會看一眼並投票。雖然每個人只懂一點點 (只切一刀)，但合起來就是全能名醫。

#### 2.3.1 深入解析：為什麼顧此失彼沒關係？ (Magic Happens Here)
你可能會問：「如果第二棒為了學新題目，結果把第一題原本會的弄錯了怎麼辦？」
**沒關係！因為我們是看最後的「加權投票」。**

**情境模擬**：現在來了一個新病人，他的腫瘤 **「很小 (像良性) 但是很硬 (像惡性)」**：
*   **第一棒 (只看大小) 說**：腫瘤很小，是 **良性** (錯)。
*   **第二棒 (只看硬度) 說**：腫瘤很硬，是 **惡性** (**對**，因為第二棒專精硬度)。
*   **第三棒 (只看形狀) 說**：形狀還好，是 **良性** (錯)。

這時候你會說：「那不是 2 票對 1 票，最後還是錯嗎？」
**不一定！這取決於 Alpha (發言權)。**

如果第二棒在訓練時，雖然只對了這類題目，但它對這類題目的信心非常強 (Alpha 很大)，而第一、三棒對此模稜兩可。最終加權分數可能是：
*   良性票：1.0 (第一棒) + 1.0 (第三棒) = **2.0 分**
*   惡性票：**2.5 (第二棒)** = **2.5 分**
*   **結果：惡性勝出！ (診斷正確)**

**結論**：
1.  **為什麼有意義？** 因為每一棒都成為了某個領域的「專家」。
    *   第一棒是「大腫瘤專家」 (簡單題)。
    *   第二棒是「硬腫瘤專家」 (中等題)。
    *   第三棒是「不規則形狀專家」 (補救題)。
2.  **顧此失彼沒關係**：第二棒雖然可能把某些「大且軟」的腫瘤判錯，但沒關係，因為第一棒會投正確的一票。只要大家合起來能覆蓋所有情況就好。
3.  **權重的作用**：它只是用來**逼迫**下一棒去學「目前還沒人學會」的東西。一旦有人學會了，這個知識就被保留在那個專家的腦袋裡了 (透過它的投票權)。

#### 2.4 什麼時候停止？
這場接力賽不會無限跑下去，通常滿足以下條件之一就會停止：
1.  **達到指定次數 (n_estimators)**：這是最常見的。我們設定 `n_estimators=50`，跑完 50 棒就強制結束。
2.  **完全學會了 (Perfect Fit)**：如果訓練資料的錯誤率變成 **0%** (全對)，那就沒東西可修正，提早下班。
3.  **學不動了**：剩下的錯題太難 (可能是雜訊)，不管怎麼切都無法改善，也會停止。
*   **注意**：特徵是**可以重複被選**的！(例如第一棒切「面積 > 10」，第十棒可能回來切「面積 < 5」)。


## 3. 實戰
### Python 程式碼實作
完整程式連結：[AdaBoost_Cancer.py](AdaBoost_Cancer.py)

```python
# 關鍵程式碼：AdaBoost

# 1. 設定弱分類器 (Decision Stump)
from sklearn.tree import DecisionTreeClassifier
weak_learner = DecisionTreeClassifier(max_depth=1)

# 2. 訓練 AdaBoost
from sklearn.ensemble import AdaBoostClassifier
# n_estimators=50: 接力 50 次
ada_model = AdaBoostClassifier(base_estimator=weak_learner, n_estimators=50, learning_rate=1.0)
ada_model.fit(X_train, y_train)
```

## 4. 模型評估與視覺化
### 1. 準確率比較 (Accuracy Comparison)
*   **弱分類器 (Stump)**：約 **89.5%** (只看一個特徵切一刀)。
*   **AdaBoost (50 Stumps)**：約 **97.4%**。
*   **結論**：透過 50 次的「知錯能改」，我們把準確率提升了近 **8%**！這在醫療診斷上是巨大的進步。

### 2. 學習曲線 (Learning Curve)
![Learning Curve](pic/19-1_Learning_Curve.png)
*   **觀察**：
    *   紅線 (AdaBoost) 一開始跟藍線 (Weak Learner) 一樣低。
    *   隨著樹的數量增加 (接力次數變多)，準確率迅速爬升。
    *   大約在 10-20 棵樹時就達到高原期 (收斂)。
*   **啟示**：這證明了 Boosting 策略的有效性。

### 3. 決策邊界 (Decision Boundary)
![Decision Boundary](pic/19-2_Decision_Boundary.png)
*   **觀察**：
    *   我們只用兩個特徵 (`mean concave points`, `worst area`) 來畫圖。
    *   背景顏色代表模型的判斷區域 (紅=惡性, 藍=良性)。
    *   可以看到邊界不再是單純的一條直線 (Stump 只能切直線)，而是呈現 **鋸齒狀** 或 **複雜的形狀**。
    *   這就是多個 Stump 疊加後的結果，能更細膩地包圍住數據點。

## 5. 戰略總結: 集成學習的火箭發射之旅

### (AdaBoost 適用)

#### 5.1 流程一：建立自信 (Weak Learner)
*   **設定**：使用簡單的模型 (如 Stump)。
*   **結果**：雖然不完美，但至少比亂猜好，建立了基礎。

#### 5.2 流程二：面對錯誤 (Reweighting)
*   **設定**：找出預測錯誤的樣本，提高它們的權重 (Weight)。
*   **結果**：模型被迫去學習那些「難搞」的案例。

#### 5.3 流程三：集思廣益 (Weighted Voting)
*   **設定**：根據每個模型的表現給予不同的發言權 (Alpha)。
*   **結果**：表現好的模型說話大聲，表現差的說話小聲，最終形成強大的決策系統。

## 6. 總結
Day 19 我們學習了 **AdaBoost**。
*   它是 Boosting 家族的開山始祖。
*   核心精神是 **「專注於錯誤」** (Focus on mistakes)。
*   它證明了簡單的模型透過正確的組合，也能達到頂尖的效果。

下一章 (Day 20)，我們將進入 Boosting 家族的完全體，也是目前 Kaggle 比賽中最統治級的算法 —— **XGBoost (Extreme Gradient Boosting)**！
